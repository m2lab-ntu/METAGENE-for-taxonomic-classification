╔══════════════════════════════════════════════════════════════════════════╗
║              METAGENE 超參數快速參考表                                     ║
╚══════════════════════════════════════════════════════════════════════════╝

📊 最重要的超參數（按影響力排序）

┌─────────────────────────────────────────────────────────────────────────┐
│ 1️⃣  tokenizer.max_length (最大序列長度)                                 │
│    • 預設: 512 (standard) / 128 (RTX 4090)                              │
│    • 範圍: 64-2048                                                      │
│    • 影響: ⚡ 記憶體使用（最大影響因素！）                                │
│    • 建議:                                                              │
│      - 短 reads (<150bp): 128-256                                      │
│      - 標準 reads (150-300bp): 256-512                                 │
│      - 長序列 (>300bp): 512-1024                                       │
│    • 記憶體: 128→13GB, 256→16GB, 512→22GB (batch_size=1)              │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ 2️⃣  lora.r (LoRA Rank - 模型容量)                                       │
│    • 預設: 8 (standard) / 4 (RTX 4090)                                  │
│    • 範圍: 1-64                                                         │
│    • 影響: 🧠 模型表達能力, ⚡ 記憶體使用                                 │
│    • 建議:                                                              │
│      - 小數據集 (<10K): r=2-4                                          │
│      - 中等數據集 (10K-100K): r=4-8                                    │
│      - 大數據集 (>100K): r=8-16                                        │
│    • 記憶體: r=2→11GB, r=4→13GB, r=8→16GB, r=16→22GB                   │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ 3️⃣  training.batch_size × grad_accum_steps (有效批次大小)               │
│    • 預設: 128×1 (standard) / 1×8 (RTX 4090)                            │
│    • 範圍: batch_size 1-512, grad_accum 1-32                           │
│    • 影響: 🎯 訓練穩定性, ⚡ 記憶體使用, ⏱️ 訓練速度                     │
│    • 建議有效批次大小:                                                   │
│      - 小數據集: 8-16                                                   │
│      - 標準: 32-64 ✅                                                   │
│      - 大數據集: 64-128                                                 │
│    • RTX 4090: batch_size=1, grad_accum=32 (有效批次=32)               │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ 4️⃣  optimizer.lr (學習率)                                               │
│    • 預設: 0.0002 (2e-4)                                                │
│    • 範圍: 1e-5 到 5e-4                                                 │
│    • 影響: 🎯 收斂速度, 🎯 最終性能                                      │
│    • 建議:                                                              │
│      - 小數據集 (<10K): 5e-5 到 1e-4                                   │
│      - 標準 (10K-100K): 1e-4 到 2e-4 ✅                                │
│      - 大數據集 (>100K): 2e-4 到 5e-4                                  │
│      - 微調已訓練模型: 1e-5 到 5e-5                                     │
│    • 症狀: 太高→震盪不穩, 太低→收斂慢                                   │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ 5️⃣  lora.target_modules (LoRA 應用的層)                                 │
│    • 預設: [q_proj, k_proj, v_proj, o_proj] / [q_proj, v_proj]        │
│    • 可選: q_proj, k_proj, v_proj, o_proj                              │
│    • 影響: 🧠 模型容量, ⚡ 記憶體使用                                     │
│    • 建議:                                                              │
│      - 記憶體充足 (>40GB): [q, k, v, o]                                │
│      - 標準 (32GB): [q, v, o]                                          │
│      - RTX 4090 (24GB): [q, v] ✅                                       │
│      - 極致省記憶體: [q]                                                 │
│    • 記憶體: [q]→-30%, [q,v]→基準, [q,v,o]→+20%, [q,k,v,o]→+40%      │
└─────────────────────────────────────────────────────────────────────────┘

════════════════════════════════════════════════════════════════════════════

🔧 其他重要超參數

┌──────────────────────────┬──────────────┬─────────────┬─────────────────┐
│ 參數                      │ 預設值       │ 範圍        │ 建議            │
├──────────────────────────┼──────────────┼─────────────┼─────────────────┤
│ lora.alpha              │ 16 / 8       │ r到2*r      │ alpha = 2*r ✅  │
│ lora.dropout            │ 0.1 / 0.05   │ 0.0-0.3     │ 0.05-0.1        │
│ model.dropout           │ 0.1          │ 0.0-0.5     │ 0.1-0.3         │
│ gradient_checkpointing  │ false / true │ bool        │ true (24GB)     │
│                         │              │             │ false (40GB+)   │
│ training.max_epochs     │ 10           │ 1-100       │ 10-20           │
│ optimizer.weight_decay  │ 0.01         │ 0.0-0.1     │ 0.01-0.05       │
│ scheduler.warmup_steps  │ 100 / 50     │ 0-1000      │ 總步數的1-5%     │
│ loss.label_smoothing    │ 0.0          │ 0.0-0.3     │ 0.0-0.1         │
│ early_stopping.patience │ 3            │ 1-10        │ 3-5             │
│ training.precision      │ bf16-mixed   │ bf16/fp16/32│ bf16-mixed ✅   │
└──────────────────────────┴──────────────┴─────────────┴─────────────────┘

════════════════════════════════════════════════════════════════════════════

💾 按 GPU 記憶體大小的推薦配置

┌────────┬────────────┬────────────┬───────┬──────────────┬──────────────┐
│ GPU    │batch_size×│max_length │lora.r │target_modules│gradient_ckpt │
│ 記憶體  │grad_accum │           │       │              │              │
├────────┼────────────┼────────────┼───────┼──────────────┼──────────────┤
│ 12GB   │ 1×32       │ 64-128     │ 2     │ [q]          │ true         │
│ 16GB   │ 1×32       │ 128-256    │ 2-4   │ [q,v]        │ true         │
│ 24GB   │ 1×32       │ 128-256    │ 4-8   │ [q,v]        │ true ✅      │
│ (4090) │            │            │       │              │              │
│ 32GB   │ 8×4        │ 256-512    │ 8     │ [q,v,o]      │ false        │
│ 40GB   │ 32×2       │ 512        │ 8-16  │ [q,k,v,o]    │ false        │
│ (A100) │            │            │       │              │              │
│ 80GB   │ 64×2       │ 512-1024   │ 16-32 │ [q,k,v,o]    │ false        │
│ (A100) │            │            │       │              │              │
└────────┴────────────┴────────────┴───────┴──────────────┴──────────────┘

════════════════════════════════════════════════════════════════════════════

📊 按數據集大小的推薦配置

┌───────────┬───────────┬────────┬─────────────┬─────────┬─────────┐
│ 數據集     │max_epochs │   lr   │weight_decay │ dropout │patience │
├───────────┼───────────┼────────┼─────────────┼─────────┼─────────┤
│ <1K       │ 50-100    │ 5e-5   │ 0.1         │ 0.3-0.5 │ 2-3     │
│ 1K-10K    │ 20-50     │ 1e-4   │ 0.05        │ 0.2-0.3 │ 3       │
│ 10K-100K  │ 10-20     │ 2e-4   │ 0.01        │ 0.1-0.2 │ 3-5 ✅  │
│ 100K-1M   │ 5-10      │ 2e-4   │ 0.01        │ 0.1     │ 3-5     │
│ >1M       │ 3-5       │ 3e-4   │ 0.001       │ 0.1     │ 5-10    │
└───────────┴───────────┴────────┴─────────────┴─────────┴─────────┘

════════════════════════════════════════════════════════════════════════════

🚨 常見問題快速診斷

問題: CUDA Out of Memory
解決: ↓ max_length (最有效！), gradient_checkpointing=true, 
     ↓ batch_size, ↓ lora.r, ↓ target_modules

問題: 訓練太慢 (<1 it/s)
解決: ↓ max_length, gradient_checkpointing=false, ↑ batch_size,
     precision=bf16-mixed

問題: 模型過擬合 (val loss ↑)
解決: ↑ dropout (0.2-0.3), ↑ weight_decay (0.05), 
     label_smoothing=0.1, early_stopping

問題: 模型欠擬合 (train/val loss 都高)
解決: ↑ lora.r, ↑ max_length, ↑ max_epochs, ↑ lr

問題: 訓練不穩定 (loss 震盪)
解決: ↓ lr, ↑ warmup_steps, precision=32, ↑ grad_accum_steps

════════════════════════════════════════════════════════════════════════════

🎯 快速啟動模板

# RTX 4090 配置（24GB）
python train.py \
  --config configs/rtx4090_optimized.yaml \
  --train_fasta data/train.fa \
  --val_fasta data/val.fa \
  --mapping_tsv data/mapping.tsv \
  --batch_size 1 \
  --lr 0.0002 \
  --max_epochs 10

# A100 配置（40GB）
python train.py \
  --config configs/default.yaml \
  --train_fasta data/train.fa \
  --val_fasta data/val.fa \
  --mapping_tsv data/mapping.tsv \
  --batch_size 32 \
  --lr 0.0002 \
  --max_epochs 10

════════════════════════════════════════════════════════════════════════════

📚 詳細文檔

完整的超參數說明: HYPERPARAMETERS_GUIDE.md
配置文件: configs/rtx4090_optimized.yaml, configs/default.yaml

════════════════════════════════════════════════════════════════════════════
