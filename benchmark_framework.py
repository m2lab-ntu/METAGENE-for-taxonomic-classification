#!/usr/bin/env python3
"""
æ€§èƒ½æ¯”è¼ƒè©•ä¼°æ¡†æ¶ (Benchmark Framework)
ç”¨æ–¼æ¨™æº–åŒ–è©•ä¼°å’Œæ¯”è¼ƒä¸åŒæ–¹æ³•çš„æ€§èƒ½

åŠŸèƒ½ (Features):
1. çµ±ä¸€çš„è©•ä¼°æŒ‡æ¨™ (Unified metrics)
2. å¤šæ–¹æ³•æ¯”è¼ƒ (Multi-method comparison)
3. è©³ç´°çš„æ€§èƒ½å ±å‘Š (Detailed performance reports)
4. å¯è¦–åŒ–çµæœ (Visualization)
"""

import os
import sys
import json
import argparse
import subprocess
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
from tqdm import tqdm


class BenchmarkEvaluator:
    """è©•ä¼°å™¨é¡"""
    
    def __init__(self, test_data_path: str, mapping_tsv: str, output_dir: str):
        self.test_data_path = test_data_path
        self.mapping_tsv = mapping_tsv
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å­˜å„²çµæœ
        self.results = {}
        
        print(f"ğŸ“Š è©•ä¼°å™¨åˆå§‹åŒ–")
        print(f"   æ¸¬è©¦æ•¸æ“š: {test_data_path}")
        print(f"   æ¨™ç±¤æ˜ å°„: {mapping_tsv}")
        print(f"   è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    def load_ground_truth(self) -> Dict[str, str]:
        """å¾æ¸¬è©¦æ•¸æ“šä¸­æå– ground truth"""
        print("\nğŸ“– è®€å– Ground Truth...")
        
        ground_truth = {}
        current_header = None
        
        with open(self.test_data_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line.startswith('>'):
                    current_header = line
                    # è§£æ: >lbl|class_id|tax_id|length|species_name
                    parts = line.strip('>').split('|')
                    if len(parts) >= 2:
                        seq_id = line
                        true_class_id = parts[1]
                        ground_truth[seq_id] = true_class_id
        
        print(f"âœ… è®€å– {len(ground_truth):,} æ¢ Ground Truth")
        return ground_truth
    
    def run_prediction(self, 
                      method_name: str,
                      model_checkpoint: str,
                      config_file: str = None,
                      script_path: str = "predict.py",
                      batch_size: int = 256) -> str:
        """
        é‹è¡Œé æ¸¬
        
        Args:
            method_name: æ–¹æ³•åç¨±
            model_checkpoint: æ¨¡å‹æª¢æŸ¥é»è·¯å¾‘
            config_file: é…ç½®æ–‡ä»¶è·¯å¾‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
            script_path: é æ¸¬è…³æœ¬è·¯å¾‘
            batch_size: Batch size
        
        Returns:
            é æ¸¬çµæœæ–‡ä»¶è·¯å¾‘
        """
        print(f"\nğŸ”® é‹è¡Œé æ¸¬: {method_name}")
        
        prediction_file = self.output_dir / f"predictions_{method_name}.csv"
        
        # æ§‹å»ºå‘½ä»¤
        cmd = [
            "python", script_path,
            "--ckpt", model_checkpoint,
            "--split", "test",
            "--input", self.test_data_path,
            "--output", str(prediction_file),
            "--batch_size", str(batch_size)
        ]
        
        if config_file:
            cmd.extend(["--config", config_file])
        
        print(f"   å‘½ä»¤: {' '.join(cmd)}")
        
        # é‹è¡Œé æ¸¬
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                check=True
            )
            print(f"âœ… é æ¸¬å®Œæˆ: {prediction_file}")
            return str(prediction_file)
        except subprocess.CalledProcessError as e:
            print(f"âŒ é æ¸¬å¤±æ•—: {e}")
            print(f"   æ¨™æº–è¼¸å‡º: {e.stdout}")
            print(f"   æ¨™æº–éŒ¯èª¤: {e.stderr}")
            return None
    
    def evaluate_predictions(self,
                            method_name: str,
                            prediction_file: str,
                            ground_truth: Dict[str, str]) -> Dict:
        """
        è©•ä¼°é æ¸¬çµæœ
        
        Args:
            method_name: æ–¹æ³•åç¨±
            prediction_file: é æ¸¬çµæœæ–‡ä»¶
            ground_truth: Ground truth å­—å…¸
        
        Returns:
            è©•ä¼°æŒ‡æ¨™å­—å…¸
        """
        print(f"\nğŸ“ˆ è©•ä¼°é æ¸¬çµæœ: {method_name}")
        
        # è®€å–é æ¸¬çµæœ
        predictions_df = pd.read_csv(prediction_file)
        
        # è¨ˆç®—æŒ‡æ¨™
        correct = 0
        total = 0
        per_class_correct = defaultdict(int)
        per_class_total = defaultdict(int)
        confidence_scores = []
        
        for _, row in predictions_df.iterrows():
            seq_id = row['sequence_id']
            predicted_class = str(row['predicted_class_id'])
            confidence = row.get('confidence', 0.0)
            
            if seq_id in ground_truth:
                true_class = ground_truth[seq_id]
                total += 1
                
                per_class_total[true_class] += 1
                confidence_scores.append(confidence)
                
                if predicted_class == true_class:
                    correct += 1
                    per_class_correct[true_class] += 1
        
        # è¨ˆç®—ç¸½é«”æº–ç¢ºç‡
        accuracy = correct / total if total > 0 else 0
        
        # è¨ˆç®—æ¯é¡æº–ç¢ºç‡
        per_class_accuracy = {}
        for class_id in per_class_total:
            per_class_accuracy[class_id] = (
                per_class_correct[class_id] / per_class_total[class_id]
                if per_class_total[class_id] > 0 else 0
            )
        
        # è¨ˆç®—å®å¹³å‡æº–ç¢ºç‡
        macro_accuracy = np.mean(list(per_class_accuracy.values())) if per_class_accuracy else 0
        
        # è¨ˆç®—åŠ æ¬Šæº–ç¢ºç‡
        weighted_accuracy = sum(
            per_class_accuracy[c] * per_class_total[c] 
            for c in per_class_accuracy
        ) / total if total > 0 else 0
        
        # å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = np.mean(confidence_scores) if confidence_scores else 0
        
        metrics = {
            'method_name': method_name,
            'total_samples': total,
            'correct_predictions': correct,
            'accuracy': accuracy,
            'macro_accuracy': macro_accuracy,
            'weighted_accuracy': weighted_accuracy,
            'average_confidence': avg_confidence,
            'num_classes': len(per_class_total),
            'per_class_accuracy': per_class_accuracy,
            'per_class_total': dict(per_class_total)
        }
        
        print(f"âœ… è©•ä¼°å®Œæˆ")
        print(f"   æº–ç¢ºç‡: {accuracy:.4f}")
        print(f"   å®å¹³å‡æº–ç¢ºç‡: {macro_accuracy:.4f}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.4f}")
        
        return metrics
    
    def compare_methods(self, methods: List[Dict]) -> pd.DataFrame:
        """
        æ¯”è¼ƒå¤šå€‹æ–¹æ³•
        
        Args:
            methods: æ–¹æ³•åˆ—è¡¨ï¼Œæ¯å€‹æ–¹æ³•åŒ…å« name, checkpoint, config ç­‰
        
        Returns:
            æ¯”è¼ƒçµæœ DataFrame
        """
        print(f"\nğŸ”¬ é–‹å§‹æ¯”è¼ƒ {len(methods)} å€‹æ–¹æ³•...")
        
        # åŠ è¼‰ ground truth
        ground_truth = self.load_ground_truth()
        
        # è©•ä¼°æ¯å€‹æ–¹æ³•
        all_results = []
        
        for method in methods:
            method_name = method['name']
            checkpoint = method['checkpoint']
            config = method.get('config', None)
            
            print(f"\n{'='*60}")
            print(f"æ–¹æ³•: {method_name}")
            print(f"{'='*60}")
            
            # é‹è¡Œé æ¸¬
            prediction_file = self.run_prediction(
                method_name=method_name,
                model_checkpoint=checkpoint,
                config_file=config
            )
            
            if prediction_file is None:
                print(f"âš ï¸  è·³éæ–¹æ³•: {method_name}")
                continue
            
            # è©•ä¼°çµæœ
            metrics = self.evaluate_predictions(
                method_name=method_name,
                prediction_file=prediction_file,
                ground_truth=ground_truth
            )
            
            all_results.append(metrics)
            self.results[method_name] = metrics
        
        # å‰µå»ºæ¯”è¼ƒè¡¨
        comparison_df = pd.DataFrame([
            {
                'Method': r['method_name'],
                'Accuracy': r['accuracy'],
                'Macro Accuracy': r['macro_accuracy'],
                'Weighted Accuracy': r['weighted_accuracy'],
                'Avg Confidence': r['average_confidence'],
                'Num Classes': r['num_classes'],
                'Total Samples': r['total_samples']
            }
            for r in all_results
        ])
        
        # æŒ‰æº–ç¢ºç‡æ’åº
        comparison_df = comparison_df.sort_values('Accuracy', ascending=False)
        
        return comparison_df
    
    def generate_report(self, comparison_df: pd.DataFrame) -> str:
        """ç”Ÿæˆè©³ç´°å ±å‘Š"""
        print(f"\nğŸ“ ç”Ÿæˆå ±å‘Š...")
        
        report_file = self.output_dir / f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# æ€§èƒ½æ¯”è¼ƒå ±å‘Š (Benchmark Report)\n\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## æ¸¬è©¦é…ç½®\n\n")
            f.write(f"- **æ¸¬è©¦æ•¸æ“š**: `{self.test_data_path}`\n")
            f.write(f"- **æ¨™ç±¤æ˜ å°„**: `{self.mapping_tsv}`\n")
            f.write(f"- **è©•ä¼°æ–¹æ³•æ•¸**: {len(comparison_df)}\n\n")
            
            f.write("## æ•´é«”æ¯”è¼ƒ\n\n")
            f.write(comparison_df.to_markdown(index=False))
            f.write("\n\n")
            
            f.write("## è©³ç´°æŒ‡æ¨™\n\n")
            
            for method_name, metrics in self.results.items():
                f.write(f"### {method_name}\n\n")
                f.write(f"- **ç¸½æ¨£æœ¬æ•¸**: {metrics['total_samples']:,}\n")
                f.write(f"- **æ­£ç¢ºé æ¸¬æ•¸**: {metrics['correct_predictions']:,}\n")
                f.write(f"- **æº–ç¢ºç‡**: {metrics['accuracy']:.4f}\n")
                f.write(f"- **å®å¹³å‡æº–ç¢ºç‡**: {metrics['macro_accuracy']:.4f}\n")
                f.write(f"- **åŠ æ¬Šæº–ç¢ºç‡**: {metrics['weighted_accuracy']:.4f}\n")
                f.write(f"- **å¹³å‡ç½®ä¿¡åº¦**: {metrics['average_confidence']:.4f}\n")
                f.write(f"- **é¡åˆ¥æ•¸**: {metrics['num_classes']}\n\n")
            
            f.write("## çµè«–\n\n")
            
            best_method = comparison_df.iloc[0]
            f.write(f"**æœ€ä½³æ–¹æ³•**: {best_method['Method']}\n")
            f.write(f"- æº–ç¢ºç‡: {best_method['Accuracy']:.4f}\n")
            f.write(f"- å®å¹³å‡æº–ç¢ºç‡: {best_method['Macro Accuracy']:.4f}\n\n")
        
        print(f"âœ… å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        # åŒæ™‚ä¿å­˜ JSON æ ¼å¼
        json_file = report_file.with_suffix('.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… JSON çµæœå·²ä¿å­˜: {json_file}")
        
        return str(report_file)
    
    def save_comparison_csv(self, comparison_df: pd.DataFrame) -> str:
        """ä¿å­˜æ¯”è¼ƒçµæœç‚º CSV"""
        csv_file = self.output_dir / "benchmark_comparison.csv"
        comparison_df.to_csv(csv_file, index=False)
        print(f"âœ… æ¯”è¼ƒçµæœå·²ä¿å­˜: {csv_file}")
        return str(csv_file)


def main():
    parser = argparse.ArgumentParser(
        description="æ€§èƒ½æ¯”è¼ƒè©•ä¼°æ¡†æ¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  python benchmark_framework.py \\
    --test_data test_data/test_full.fa \\
    --mapping_tsv species_mapping_converted.tsv \\
    --output_dir benchmark_results \\
    --methods methods_config.json
        """
    )
    
    parser.add_argument('--test_data', required=True,
                       help='æ¸¬è©¦æ•¸æ“šè·¯å¾‘')
    parser.add_argument('--mapping_tsv', required=True,
                       help='æ¨™ç±¤æ˜ å°„æ–‡ä»¶')
    parser.add_argument('--output_dir', required=True,
                       help='è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--methods', required=True,
                       help='æ–¹æ³•é…ç½® JSON æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # è®€å–æ–¹æ³•é…ç½®
    with open(args.methods, 'r') as f:
        methods = json.load(f)
    
    # å‰µå»ºè©•ä¼°å™¨
    evaluator = BenchmarkEvaluator(
        test_data_path=args.test_data,
        mapping_tsv=args.mapping_tsv,
        output_dir=args.output_dir
    )
    
    # æ¯”è¼ƒæ–¹æ³•
    comparison_df = evaluator.compare_methods(methods)
    
    # ç”Ÿæˆå ±å‘Š
    evaluator.generate_report(comparison_df)
    evaluator.save_comparison_csv(comparison_df)
    
    print(f"\n{'='*60}")
    print("ğŸ‰ è©•ä¼°å®Œæˆï¼")
    print(f"{'='*60}\n")
    print(comparison_df.to_string(index=False))


if __name__ == '__main__':
    main()

