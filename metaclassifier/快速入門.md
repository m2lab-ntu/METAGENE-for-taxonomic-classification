# MetaClassifier å¿«é€Ÿå…¥é–€ (Quick Start)

## ğŸ¯ 5 åˆ†é˜å¿«é€Ÿä¸Šæ‰‹

### **ç¬¬ 1 æ­¥ï¼šæº–å‚™é…ç½®æ–‡ä»¶**

è¤‡è£½ç¤ºä¾‹é…ç½®ï¼š

```bash
cd /media/user/disk2/METAGENE/classification/metaclassifier

# è¤‡è£½é…ç½®æ–‡ä»¶
cp configs/metagene_bpe.yaml configs/my_training.yaml
```

### **ç¬¬ 2 æ­¥ï¼šç·¨è¼¯é…ç½®æ–‡ä»¶**

ç·¨è¼¯ `configs/my_training.yaml`ï¼Œ**åªéœ€ä¿®æ”¹é€™ 3 å€‹åœ°æ–¹ï¼š**

```yaml
# 1. ä¿®æ”¹æ•¸æ“šè·¯å¾‘ âš ï¸ å¿…é ˆä¿®æ”¹
dataset:
  train_fasta: /media/user/disk2/full_labeled_species_train_reads_shuffled
  val_fasta: /media/user/disk2/full_labeled_species_val_reads_shuffled
  mapping_tsv: /media/user/disk2/MetaTransformer_new_pipeline/myScript/all_available_species_mapping.tab

# 2. ä¿®æ”¹è¼¸å‡ºç›®éŒ„
training:
  output_dir: outputs/my_first_metaclassifier_run

# 3. ï¼ˆå¯é¸ï¼‰èª¿æ•´åºåˆ—é•·åº¦
tokenizer:
  max_length: 192  # 150bp è®€é•·æ¨è–¦ 192
```

### **ç¬¬ 3 æ­¥ï¼šé–‹å§‹è¨“ç·´**

```bash
# æ¿€æ´»ç’°å¢ƒ
source /home/user/anaconda3/bin/activate METAGENE

# è¨­ç½®ç·©å­˜ç›®éŒ„
export HF_HOME=/media/user/disk2/.cache/huggingface
export TRANSFORMERS_CACHE=/media/user/disk2/.cache/huggingface
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128

# é–‹å§‹è¨“ç·´
python train.py --config configs/my_training.yaml
```

**å®Œæˆï¼** ğŸ‰ è¨“ç·´æœƒè‡ªå‹•é–‹å§‹ï¼Œä¸¦ä¿å­˜åˆ° `outputs/my_first_metaclassifier_run/`

---

## ğŸ“‹ å®Œæ•´é…ç½®æ¨¡æ¿

å¦‚æœæ‚¨æƒ³å¾é›¶é–‹å§‹ï¼Œé€™æ˜¯ä¸€å€‹å®Œæ•´çš„é…ç½®æ¨¡æ¿ï¼š

```yaml
# MetaClassifier é…ç½®æ–‡ä»¶æ¨¡æ¿

# ===== Tokenizer é…ç½® =====
tokenizer:
  type: bpe                           # é¸é …: bpe, kmer, evo2
  path: "metagene-ai/METAGENE-1"     # æ¨¡å‹è·¯å¾‘
  max_length: 192                     # æœ€å¤§åºåˆ—é•·åº¦ï¼ˆ150bp è®€é•·æ¨è–¦ï¼‰

# ===== Encoder é…ç½® =====
encoder:
  type: metagene                      # é¸é …: metagene, dnabert, evo2
  path: "metagene-ai/METAGENE-1"     # æ¨¡å‹è·¯å¾‘
  freeze: false                       # false=å¾®èª¿, true=å‡çµï¼ˆåƒ…ç‰¹å¾µæå–ï¼‰
  gradient_checkpointing: true        # ç¯€çœ GPU å…§å­˜
  
  # LoRA é…ç½®ï¼ˆåƒæ•¸é«˜æ•ˆå¾®èª¿ï¼‰
  lora:
    enabled: true
    r: 4                              # LoRA rank (4-8)
    alpha: 8                          # LoRA alpha
    dropout: 0.05
    target_modules: [q_proj, v_proj]  # ç›®æ¨™æ¨¡å¡Š
    bias: none

# ===== Model é…ç½® =====
model:
  pooling: mean                       # é¸é …: mean, cls, max
  classifier_type: linear             # é¸é …: linear, transformer
  classifier_config:
    dropout: 0.1

# ===== æ•¸æ“šé›†é…ç½® =====
dataset:
  train_fasta: /path/to/train.fasta  # âš ï¸ ä¿®æ”¹ç‚ºæ‚¨çš„è·¯å¾‘
  val_fasta: /path/to/val.fasta      # âš ï¸ ä¿®æ”¹ç‚ºæ‚¨çš„è·¯å¾‘
  mapping_tsv: /path/to/mapping.tsv  # âš ï¸ ä¿®æ”¹ç‚ºæ‚¨çš„è·¯å¾‘
  
  # Header æ ¼å¼ï¼ˆæ ¹æ“šæ‚¨çš„æ•¸æ“šèª¿æ•´ï¼‰
  header_regex: "^lbl\\|(?P<class_id>\\d+)\\|(?P<tax_id>\\d+)?\\|(?P<readlen>\\d+)?\\|(?P<name>[^/\\s]+)(?:/(?P<mate>\\d+))?$"
  
  # åˆ—å
  class_column: class_id
  label_column: label_name
  tax_column: tax_id
  
  # å…¶ä»–
  file_format: auto
  gzip: auto
  strict_classes: true

# ===== è¨“ç·´é…ç½® =====
training:
  output_dir: outputs/my_training     # è¼¸å‡ºç›®éŒ„
  batch_size: 1                       # Batch size
  grad_accum_steps: 8                 # æ¢¯åº¦ç´¯ç©æ­¥æ•¸
  max_epochs: 5                       # æœ€å¤§ epoch æ•¸
  precision: bf16-mixed               # æ··åˆç²¾åº¦
  
  # æ—©åœ
  early_stopping:
    patience: 2                       # ä¸æ”¹å–„å¤šå°‘ epoch å¾Œåœæ­¢
    metric: macro_f1                  # ç›£æ§æŒ‡æ¨™
    mode: max                         # max æˆ– min
  
  # æª¢æŸ¥é»
  checkpoint:
    save_best: true
    save_last: true
  
  torch_compile: false

# ===== å„ªåŒ–å™¨é…ç½® =====
optimizer:
  name: adamw
  lr: 0.0002                          # å­¸ç¿’ç‡
  weight_decay: 0.01
  betas: [0.9, 0.999]
  use_8bit: false

# ===== å­¸ç¿’ç‡èª¿åº¦å™¨ =====
scheduler:
  name: linear                        # é¸é …: linear, cosine
  warmup_steps: 50
  num_training_steps: auto

# ===== æå¤±å‡½æ•¸ =====
loss:
  name: cross_entropy
  label_smoothing: 0.0

# ===== è©•ä¼°æŒ‡æ¨™ =====
metrics:
  primary: macro_f1
  compute_auroc: true
  confusion_matrix: true
  per_class_report: true

# ===== æ—¥èªŒé…ç½® =====
logging:
  log_interval: 5
  use_wandb: false
  wandb_project: metaclassifier
  wandb_entity: null

# ===== æ•¸æ“šåŠ è¼‰å™¨ =====
data_loader:
  num_workers: 4
  prefetch_factor: 2
```

---

## ğŸ”§ å¸¸ç”¨é…ç½®çµ„åˆ

### **é…ç½® 1: å¿«é€Ÿæ¸¬è©¦ï¼ˆä½ç²¾åº¦ï¼Œå¿«é€Ÿï¼‰**

```yaml
tokenizer:
  max_length: 128                # è¼ƒçŸ­çš„åºåˆ—

encoder:
  lora:
    r: 4                         # å° LoRA

training:
  batch_size: 2                  # å¤§ batch
  grad_accum_steps: 4            # å°‘ç´¯ç©
  max_epochs: 3                  # å°‘ epoch
```

**é æœŸæ™‚é–“ï¼š** ~2-3 å¤©

---

### **é…ç½® 2: å¹³è¡¡ï¼ˆä¸­ç­‰ç²¾åº¦ï¼Œåˆç†é€Ÿåº¦ï¼‰** âœ… æ¨è–¦

```yaml
tokenizer:
  max_length: 192                # å®Œæ•´è®€é•·

encoder:
  lora:
    r: 4                         # ä¸­ç­‰ LoRA

training:
  batch_size: 1                  # æ¨™æº– batch
  grad_accum_steps: 8            # æ¨™æº–ç´¯ç©
  max_epochs: 5                  # ä¸­ç­‰ epoch
```

**é æœŸæ™‚é–“ï¼š** ~4-5 å¤©

---

### **é…ç½® 3: é«˜ç²¾åº¦ï¼ˆæœ€é«˜ç²¾åº¦ï¼Œè¼ƒæ…¢ï¼‰**

```yaml
tokenizer:
  max_length: 192                # å®Œæ•´è®€é•·

encoder:
  lora:
    r: 8                         # å¤§ LoRA
    alpha: 16

training:
  batch_size: 1                  # å° batch
  grad_accum_steps: 16           # å¤šç´¯ç©
  max_epochs: 10                 # å¤š epoch
```

**é æœŸæ™‚é–“ï¼š** ~8-10 å¤©

---

## ğŸ“Š é æ¸¬ä½¿ç”¨

è¨“ç·´å®Œæˆå¾Œï¼Œä½¿ç”¨æ¨¡å‹é€²è¡Œé æ¸¬ï¼š

### **åŸºæœ¬é æ¸¬ï¼ˆå–®è®€å±¤ç´šï¼‰**

```bash
python predict.py \
  --config configs/my_training.yaml \
  --checkpoint outputs/my_first_metaclassifier_run/checkpoints/best.pt \
  --input /path/to/test_reads.fasta \
  --output predictions.csv \
  --batch_size 256
```

**è¼¸å‡ºï¼š** `predictions.csv` - æ¯æ¢è®€çš„åˆ†é¡çµæœ

---

### **é æ¸¬ + æ¨£æœ¬èšåˆï¼ˆè¨ˆç®—ç›¸å°è±åº¦ï¼‰**

```bash
python predict.py \
  --config configs/my_training.yaml \
  --checkpoint outputs/my_first_metaclassifier_run/checkpoints/best.pt \
  --input /path/to/test_reads.fasta \
  --output predictions.csv \
  --aggregate \
  --abundance_output abundance.csv \
  --confidence_threshold 0.7
```

**è¼¸å‡ºï¼š**
- `predictions.csv` - æ¯æ¢è®€çš„åˆ†é¡
- `abundance.csv` - æ¯å€‹æ¨£æœ¬çš„ç‰©ç¨®ç›¸å°è±åº¦
- `diversity_metrics_abundance.csv` - Shannon/Simpson å¤šæ¨£æ€§æŒ‡æ•¸

---

## ğŸ›ï¸ å¯¦é©—ä¸åŒé…ç½®

MetaClassifier çš„å„ªå‹¢åœ¨æ–¼**è¼•é¬†åˆ‡æ›ä¸åŒçµ„ä»¶**ï¼š

### **å¯¦é©— 1: æ›ç”¨ k-mer tokenizer**

```yaml
tokenizer:
  type: kmer         # â† æ”¹é€™è£¡
  k: 6
  overlap: true
  max_length: 512
```

### **å¯¦é©— 2: æ›ç”¨ DNABERT**

```yaml
tokenizer:
  type: kmer         # DNABERT ç”¨ k-mer
  k: 6

encoder:
  type: dnabert      # â† æ”¹é€™è£¡
  path: "zhihan1996/DNABERT-2-117M"
```

### **å¯¦é©— 3: ä½¿ç”¨ Transformer classifier**

```yaml
model:
  classifier_type: transformer  # â† æ”¹é€™è£¡
  classifier_config:
    num_layers: 2
    num_heads: 8
    dim_feedforward: 2048
```

---

## âš ï¸ æ•…éšœæ’é™¤

### **å•é¡Œ 1: GPU OOM (å…§å­˜ä¸è¶³)**

```yaml
encoder:
  gradient_checkpointing: true    # å•Ÿç”¨

training:
  batch_size: 1                    # æ¸›å°
  grad_accum_steps: 16             # å¢åŠ 
```

### **å•é¡Œ 2: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶**

ç¢ºä¿è¨­ç½®ç·©å­˜è·¯å¾‘ï¼š
```bash
export HF_HOME=/media/user/disk2/.cache/huggingface
export TRANSFORMERS_CACHE=/media/user/disk2/.cache/huggingface
```

### **å•é¡Œ 3: Header regex ä¸åŒ¹é…**

æª¢æŸ¥æ‚¨çš„ FASTA header æ ¼å¼ï¼š
```bash
head -n 2 /path/to/your/train.fasta
```

æ ¹æ“šå¯¦éš›æ ¼å¼ä¿®æ”¹ `header_regex`ã€‚

---

## ğŸ“š ä¸‹ä¸€æ­¥

1. **é–±è®€è©³ç´°æ–‡æª”ï¼š**
   - `é…ç½®ä¿®æ”¹æŒ‡å—.md` - è©³ç´°çš„é…ç½®èªªæ˜
   - `ç³»çµ±å°æ¯”èªªæ˜.md` - èˆ‡ç¾æœ‰ç³»çµ±çš„å°æ¯”
   - `ARCHITECTURE.md` - æ¶æ§‹è¨­è¨ˆ

2. **æŸ¥çœ‹ç¤ºä¾‹ï¼š**
   ```bash
   ls examples/
   # basic_usage.py
   # compare_tokenizers.py
   # sample_aggregation.py
   ```

3. **é–‹å§‹å¯¦é©—ï¼š**
   - å˜—è©¦ä¸åŒçš„ tokenizer
   - æ¯”è¼ƒä¸åŒçš„æ¨¡å‹
   - èª¿æ•´è¶…åƒæ•¸

---

## ğŸ‰ æ­å–œï¼

æ‚¨ç¾åœ¨å·²ç¶“æŒæ¡äº† MetaClassifier çš„åŸºæœ¬ä½¿ç”¨ï¼

**å¿«é€Ÿå›é¡§ï¼š**
1. âœ… è¤‡è£½é…ç½®æ–‡ä»¶
2. âœ… ä¿®æ”¹æ•¸æ“šè·¯å¾‘
3. âœ… é‹è¡Œ `train.py`
4. âœ… ä½¿ç”¨ `predict.py` é€²è¡Œé æ¸¬

**éœ€è¦å¹«åŠ©ï¼Ÿ**
- æŸ¥çœ‹ `é…ç½®ä¿®æ”¹æŒ‡å—.md`
- é–±è®€ `ç³»çµ±å°æ¯”èªªæ˜.md`
- åƒè€ƒ `examples/` ç›®éŒ„

