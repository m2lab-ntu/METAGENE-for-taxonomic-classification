# MetaClassifier Architecture

## ðŸ—ï¸ Design Principles

1. **Modularity**: Each component (tokenizer, encoder, classifier) is independent
2. **Pluggability**: Easily swap components without changing core logic
3. **Extensibility**: Add new tokenizers/encoders by inheriting base classes
4. **Configurability**: All settings via YAML files
5. **Scalability**: Support batch processing and distributed inference

---

## ðŸ“¦ Module Structure

```
metaclassifier/
â”‚
â”œâ”€â”€ tokenization/              # Layer 1: Tokenization
â”‚   â”œâ”€â”€ base.py               # Abstract base class
â”‚   â”œâ”€â”€ bpe_tokenizer.py      # BPE implementation
â”‚   â”œâ”€â”€ kmer_tokenizer.py     # K-mer implementation
â”‚   â””â”€â”€ evo2_tokenizer.py     # Single-nucleotide implementation
â”‚
â”œâ”€â”€ embedding/                 # Layer 2: Encoding
â”‚   â”œâ”€â”€ base.py               # Abstract base class
â”‚   â”œâ”€â”€ metagene_encoder.py   # METAGENE-1 wrapper
â”‚   â”œâ”€â”€ evo2_encoder.py       # Evo2 wrapper (placeholder)
â”‚   â””â”€â”€ dnabert_encoder.py    # DNABERT wrapper
â”‚
â”œâ”€â”€ model/                     # Layer 3: Classification
â”‚   â”œâ”€â”€ pooling.py            # Pooling strategies
â”‚   â”œâ”€â”€ head.py               # Classifier heads
â”‚   â””â”€â”€ classifier.py         # Complete model
â”‚
â”œâ”€â”€ data/                      # Data utilities (future)
â”œâ”€â”€ utils/                     # Helper functions (future)
â”‚
â”œâ”€â”€ predict.py                 # Inference script
â”œâ”€â”€ aggregate.py               # Sample aggregation
â”‚
â”œâ”€â”€ configs/                   # Configuration files
â”‚   â”œâ”€â”€ metagene_bpe.yaml
â”‚   â””â”€â”€ dnabert_kmer.yaml
â”‚
â””â”€â”€ __init__.py               # Package exports
```

---

## ðŸ”„ Data Flow

```
1. INPUT: DNA Sequence (FASTA/FASTQ)
   "ATCGATCGATCG..."
         â†“
         
2. TOKENIZATION (Layer 1)
   BaseTokenizer â†’ BPETokenizer/KmerTokenizer/Evo2Tokenizer
   Output: Token IDs [1042, 543, 234, ...]
         â†“
         
3. ENCODING (Layer 2)
   BaseEncoder â†’ MetageneEncoder/DNABERTEncoder/Evo2Encoder
   Output: Hidden States (batch_size, seq_len, hidden_dim)
         â†“
         
4. POOLING (Layer 3a)
   MeanPooling/CLSPooling/MaxPooling
   Output: Pooled Embeddings (batch_size, hidden_dim)
         â†“
         
5. CLASSIFICATION (Layer 3b)
   LinearHead/TransformerHead
   Output: Logits (batch_size, num_classes)
         â†“
         
6. PREDICTION
   argmax(softmax(logits))
   Output: Class IDs + Probabilities
         â†“
         
7. AGGREGATION (Optional)
   aggregate.py â†’ Per-Sample Abundance
   Output: {sample_id: {species: abundance}}
```

---

## ðŸ§© Component Interfaces

### 1. Tokenizer Interface

```python
class BaseTokenizer(ABC):
    @abstractmethod
    def tokenize(sequence: str) -> List[str]:
        """Convert sequence to tokens"""
        
    @abstractmethod
    def encode(sequence: str) -> List[int]:
        """Convert sequence to token IDs"""
        
    def pad_and_truncate(token_ids: List[int]) -> List[int]:
        """Pad/truncate to max_length"""
        
    def create_attention_mask(token_ids: List[int]) -> List[int]:
        """Create attention mask"""
```

**Implementations**:
- `BPETokenizer`: Byte-pair encoding (METAGENE-1 style)
- `KmerTokenizer`: K-mer tokenization (overlapping/non-overlapping)
- `Evo2Tokenizer`: Single-nucleotide tokens

---

### 2. Encoder Interface

```python
class BaseEncoder(nn.Module, ABC):
    @abstractmethod
    def forward(input_ids, attention_mask) -> Tensor:
        """Encode tokens to hidden states"""
        
    @abstractmethod
    def get_embedding_dim() -> int:
        """Get hidden dimension"""
        
    def freeze_encoder():
        """Freeze for feature extraction"""
        
    def unfreeze_encoder():
        """Unfreeze for fine-tuning"""
```

**Implementations**:
- `MetageneEncoder`: METAGENE-1 7B parameter model
- `DNABERTEncoder`: DNABERT/DNABERT-2
- `Evo2Encoder`: Evo model (placeholder)

---

### 3. Classifier Interface

```python
class TaxonomicClassifier(nn.Module):
    def __init__(
        encoder: BaseEncoder,
        num_classes: int,
        pooling_strategy: str,
        classifier_type: str
    ):
        """Initialize with encoder and config"""
        
    def forward(input_ids, attention_mask, labels=None) -> Dict:
        """Full forward pass"""
        
    def predict(input_ids, attention_mask) -> Tensor:
        """Make predictions"""
        
    def get_embeddings(input_ids, attention_mask) -> Tensor:
        """Extract embeddings"""
```

**Pooling Options**:
- `MeanPooling`: Average over sequence
- `CLSPooling`: First token (CLS)
- `MaxPooling`: Max over sequence

**Classifier Types**:
- `LinearClassifierHead`: Simple linear layer
- `TransformerClassifierHead`: Transformer + linear (MetaTransformer-style)

---

## ðŸ”Œ Extending the Pipeline

### Adding a New Tokenizer

```python
# tokenization/my_tokenizer.py

from .base import BaseTokenizer

class MyTokenizer(BaseTokenizer):
    def __init__(self, my_param, max_length=512):
        super().__init__(max_length)
        self.my_param = my_param
        
    def tokenize(self, sequence: str) -> List[str]:
        # Your tokenization logic
        return tokens
        
    def encode(self, sequence: str) -> List[int]:
        tokens = self.tokenize(sequence)
        # Convert to IDs
        return token_ids
        
    def get_vocab_size(self) -> int:
        return len(self.vocab)
```

Then add to `tokenization/__init__.py`:
```python
from .my_tokenizer import MyTokenizer
__all__ = [..., 'MyTokenizer']
```

---

### Adding a New Encoder

```python
# embedding/my_encoder.py

from .base import BaseEncoder
import torch.nn as nn

class MyEncoder(BaseEncoder):
    def __init__(self, model_path, freeze=False):
        # Load your model
        model = load_my_model(model_path)
        hidden_size = model.config.hidden_size
        
        super().__init__(model_path, hidden_size, freeze)
        self.encoder = model
        
    def forward(self, input_ids, attention_mask):
        outputs = self.encoder(input_ids, attention_mask)
        return outputs.last_hidden_state
        
    def get_embedding_dim(self) -> int:
        return self.hidden_size
```

---

### Adding a New Classifier Head

```python
# model/head.py

class MyClassifierHead(nn.Module):
    def __init__(self, hidden_size, num_classes):
        super().__init__()
        # Your classifier architecture
        
    def forward(self, pooled_output):
        # Classification logic
        return logits
```

---

## ðŸ“Š Configuration Schema

```yaml
# Full configuration schema

tokenizer:
  type: str              # bpe, kmer, evo2, my_tokenizer
  path: str              # Model path or identifier
  max_length: int        # Max sequence length
  # Type-specific params
  k: int                 # For kmer
  overlap: bool          # For kmer
  use_hf: bool           # For bpe

encoder:
  type: str              # metagene, dnabert, evo2, my_encoder
  path: str              # Model path
  freeze: bool           # Freeze weights
  lora:                  # LoRA config (optional)
    enabled: bool
    r: int
    alpha: int
    dropout: float
    target_modules: list
    bias: str

model:
  pooling: str           # mean, cls, max
  classifier_type: str   # linear, transformer
  classifier_config:     # Classifier-specific config
    dropout: float
    num_layers: int      # For transformer
    num_heads: int       # For transformer

training:               # Training config (optional)
  batch_size: int
  max_epochs: int
  lr: float

prediction:             # Prediction config (optional)
  batch_size: int
  confidence_threshold: float
```

---

## ðŸŽ¯ Design Patterns

### 1. Strategy Pattern

Different tokenizers/encoders implement the same interface:

```python
# Client code doesn't know which tokenizer is used
tokenizer = create_tokenizer(config['tokenizer'])
tokens = tokenizer.encode(sequence)
```

### 2. Factory Pattern

Creating components from config:

```python
def create_tokenizer(config):
    tokenizer_type = config['type']
    if tokenizer_type == 'bpe':
        return BPETokenizer(...)
    elif tokenizer_type == 'kmer':
        return KmerTokenizer(...)
    # etc.
```

### 3. Composition Pattern

Model is composed of pluggable components:

```python
model = TaxonomicClassifier(
    encoder=encoder,      # Pluggable
    pooling=pooling,      # Pluggable
    classifier=head       # Pluggable
)
```

---

## ðŸ”¬ Testing Strategy

```
tests/
â”œâ”€â”€ test_tokenization/
â”‚   â”œâ”€â”€ test_bpe.py
â”‚   â”œâ”€â”€ test_kmer.py
â”‚   â””â”€â”€ test_evo2.py
â”‚
â”œâ”€â”€ test_embedding/
â”‚   â”œâ”€â”€ test_metagene.py
â”‚   â”œâ”€â”€ test_dnabert.py
â”‚   â””â”€â”€ test_base.py
â”‚
â”œâ”€â”€ test_model/
â”‚   â”œâ”€â”€ test_pooling.py
â”‚   â”œâ”€â”€ test_heads.py
â”‚   â””â”€â”€ test_classifier.py
â”‚
â””â”€â”€ test_integration/
    â”œâ”€â”€ test_end_to_end.py
    â””â”€â”€ test_prediction.py
```

---

## ðŸ“ˆ Performance Considerations

1. **Memory**: Use gradient checkpointing for large models
2. **Speed**: Batch processing, mixed precision (bf16/fp16)
3. **Scalability**: DataLoader with multiple workers
4. **Inference**: Model quantization, ONNX export

---

## ðŸš€ Future Enhancements

1. **Multi-GPU**: DistributedDataParallel support
2. **Streaming**: Process large FASTQ files without loading all into memory
3. **Caching**: Cache encoder outputs for faster re-training
4. **Quantization**: INT8 quantization for faster inference
5. **Multi-task**: Predict multiple taxonomic levels simultaneously
6. **Hybrid models**: Combine multiple encoders

---

## ðŸ“š References

- **METAGENE-1**: https://arxiv.org/abs/2410.03461
- **DNABERT-2**: https://arxiv.org/abs/2306.15006
- **Evo**: https://arxiv.org/abs/2403.11389
- **LoRA**: https://arxiv.org/abs/2106.09685

---

This architecture provides a solid foundation for taxonomic classification while remaining flexible and extensible for future research directions.

