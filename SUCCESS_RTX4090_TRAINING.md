# âœ… æˆåŠŸï¼RTX 4090 è¨“ç·´ METAGENE-1 Classification

## ğŸ‰ é‡å¤§çªç ´

**RTX 4090 (24GB) ç¾åœ¨å¯ä»¥æˆåŠŸè¨“ç·´ METAGENE-1 (7B åƒæ•¸)ï¼**

## ğŸ“Š æ¸¬è©¦çµæœ

### è¨˜æ†¶é«”ä½¿ç”¨
- **å³°å€¼ GPU ä½¿ç”¨**: **13.0GB / 24GB** âœ“
- **è¨“ç·´æ™‚é–“**: ~3åˆ†é˜ï¼ˆ9å€‹æ¨£æœ¬ï¼Œ1å€‹epochï¼‰
- **ç‹€æ…‹**: âœ… **ç„¡ OOM éŒ¯èª¤ï¼**

### è¨“ç·´çµ±è¨ˆ
```
Epoch 1/1:
- Train Loss: 1.1012
- Train Accuracy: 33.33%
- Train Macro F1: 33.33%
- Val Accuracy: 33.33%
- Val Macro F1: 16.67%
- Training Speed: ~3.79 it/s
```

### æ¨¡å‹åƒæ•¸
```
Total Parameters:     6,482,575,363
Trainable Parameters: 2,109,443
Trainable Ratio:      0.03%
```

## ğŸ”§ æˆåŠŸçš„å„ªåŒ–ç­–ç•¥

### 1. **Gradient Checkpointing** â­ (æœ€é—œéµ)
```yaml
model:
  gradient_checkpointing: true
```
**æ•ˆæœ**: ç¯€çœ ~50% activation memory  
**æ¬Šè¡¡**: è¨“ç·´é€Ÿåº¦é™ä½ ~15-20%

### 2. **æ¸›å°‘åºåˆ—é•·åº¦**
```yaml
tokenizer:
  max_length: 128  # å¾ 512 é™è‡³ 128
```
**æ•ˆæœ**: ç¯€çœ ~60% sequence memory  
**æ¬Šè¡¡**: é•·åºåˆ—æœƒè¢«æˆªæ–·

### 3. **æ›´å°çš„ LoRA Rank**
```yaml
model:
  lora:
    r: 4  # å¾ 8 é™è‡³ 4
    alpha: 8
```
**æ•ˆæœ**: ç¯€çœ ~50% LoRA parameters  
**æ¬Šè¡¡**: æ¨¡å‹è¡¨é”èƒ½åŠ›ç•¥é™ï¼ˆé€šå¸¸ <2%ï¼‰

### 4. **æ¸›å°‘ Target Modules**
```yaml
model:
  lora:
    target_modules: [q_proj, v_proj]  # åªè¨“ç·´ Q å’Œ V
```
**æ•ˆæœ**: ç¯€çœ ~50% adapter memory  
**æ¬Šè¡¡**: ç•¥å¾®é™ä½å¾®èª¿éˆæ´»æ€§

### 5. **Gradient Accumulation**
```yaml
training:
  batch_size: 1
  grad_accum_steps: 8  # æœ‰æ•ˆ batch size = 8
```
**æ•ˆæœ**: å…è¨±å° batch size åŒæ™‚ä¿æŒè¨“ç·´ç©©å®šæ€§  
**æ¬Šè¡¡**: è¨“ç·´é€Ÿåº¦ç•¥æ…¢

### 6. **å®šæœŸè¨˜æ†¶é«”æ¸…ç†**
```yaml
memory_optimization:
  empty_cache_steps: 10
```
**æ•ˆæœ**: æ¸›å°‘è¨˜æ†¶é«”ç¢ç‰‡  
**æ¬Šè¡¡**: è¼•å¾®æ€§èƒ½é–‹éŠ·

### 7. **è¨˜æ†¶é«”åˆ†é…å„ªåŒ–**
```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
```
**æ•ˆæœ**: æ›´å¥½çš„è¨˜æ†¶é«”ç®¡ç†  
**æ¬Šè¡¡**: ç„¡

## ğŸ“ ç”Ÿæˆçš„æª”æ¡ˆ

```
outputs/optimized_test/
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ best.pt                          # æœ€ä½³æ¨¡å‹
â”œâ”€â”€ final_model/
â”‚   â”œâ”€â”€ model.safetensors                # æœ€çµ‚æ¬Šé‡
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ label2id.json
â”‚   â””â”€â”€ id2label.json
â”œâ”€â”€ plots/
â”‚   â””â”€â”€ training_curves.png              # è¨“ç·´æ›²ç·š
â”œâ”€â”€ config.json                          # è¨“ç·´é…ç½®
â”œâ”€â”€ final_metrics.json                   # æœ€çµ‚æŒ‡æ¨™
â”œâ”€â”€ train_class_distribution.csv
â””â”€â”€ val_class_distribution.csv
```

## ğŸš€ å¦‚ä½•ä½¿ç”¨

### å¿«é€Ÿé–‹å§‹

```bash
cd /media/user/disk2/METAGENE/classification

# 1. è¨­ç½®ç’°å¢ƒ
source setup_env.sh

# 2. é‹è¡Œæ¸¬è©¦
bash test_optimized_training.sh

# 3. è¨“ç·´ä½ çš„è³‡æ–™
python train.py \
  --config configs/rtx4090_optimized.yaml \
  --train_fasta /path/to/your/train.fa \
  --val_fasta /path/to/your/val.fa \
  --mapping_tsv /path/to/your/mapping.tsv \
  --output_dir outputs/my_experiment \
  --max_epochs 10
```

### ç”¨ä½ çš„çœŸå¯¦è³‡æ–™è¨“ç·´

```bash
# ä½¿ç”¨ full_labeled_species è³‡æ–™
export HF_HOME=/media/user/disk2/.cache/huggingface
export TRANSFORMERS_CACHE=/media/user/disk2/.cache/huggingface
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128

conda activate METAGENE

python train.py \
  --config configs/rtx4090_optimized.yaml \
  --train_fasta /media/user/disk2/full_labeled_species_train_reads/train_reads.fa \
  --val_fasta /media/user/disk2/full_labeled_species_val_reads/val_reads.fa \
  --mapping_tsv /media/user/disk2/MetaTransformer_new_pipeline/myScript/all_available_species_mapping.tab \
  --output_dir outputs/species_classification_optimized \
  --batch_size 1 \
  --max_epochs 10
```

## âš™ï¸ é…ç½®èª¿æ•´å»ºè­°

### å¦‚æœæƒ³è¦æ›´å¿«çš„è¨“ç·´

```yaml
# configs/rtx4090_faster.yaml
training:
  batch_size: 2  # å¯èƒ½éœ€è¦æ¸›å°‘ max_length
  grad_accum_steps: 4  # æœ‰æ•ˆ batch = 8

tokenizer:
  max_length: 96  # æ›´çŸ­ä»¥å…è¨± batch_size=2
```

### å¦‚æœæƒ³è¦æ›´å¥½çš„æº–ç¢ºåº¦

```yaml
# configs/rtx4090_quality.yaml
model:
  lora:
    r: 8  # å¢åŠ  rank
    target_modules: [q_proj, k_proj, v_proj, o_proj]  # æ‰€æœ‰æ¨¡çµ„

tokenizer:
  max_length: 256  # æ›´é•·åºåˆ—ï¼ˆä½† batch_size å¿…é ˆ = 1ï¼‰
```

### å¦‚æœé‚„æ˜¯ OOM

```yaml
# configs/rtx4090_ultra_safe.yaml
tokenizer:
  max_length: 64  # æ¥µçŸ­åºåˆ—

model:
  lora:
    r: 2  # æœ€å° rank
    target_modules: [q_proj]  # åªè¨“ç·´ Q

training:
  precision: fp16-mixed  # æœ‰æ™‚æ¯” bf16 æ›´çœè¨˜æ†¶é«”
```

## ğŸ“ˆ é æœŸè¨“ç·´æ™‚é–“

åŸºæ–¼æ¸¬è©¦çµæœï¼ˆ9å€‹æ¨£æœ¬ï¼Œ1 epoch = 3åˆ†é˜ï¼‰ï¼š

| Dataset Size | Epochs | Estimated Time |
|-------------|--------|----------------|
| 1,000 reads | 10 | ~30 åˆ†é˜ |
| 10,000 reads | 10 | ~5 å°æ™‚ |
| 100,000 reads | 10 | ~50 å°æ™‚ |
| 1,000,000 reads | 10 | ~500 å°æ™‚ |

**å»ºè­°**ï¼š
- å°æ–¼å¤§è³‡æ–™é›†ï¼ˆ100k+ï¼‰ï¼Œè€ƒæ…®æ¸›å°‘ epochs æˆ–ä½¿ç”¨æ›´å¤§çš„ GPU
- æˆ–è€…ä½¿ç”¨ `grad_accum_steps=16` ä¾†åŠ å¿«é€Ÿåº¦

## ğŸ¯ æ•ˆèƒ½æ¯”è¼ƒ

| é…ç½® | GPU Memory | Batch Size | Speed | å¯è¡Œæ€§ |
|------|------------|-----------|-------|--------|
| **åŸå§‹ (512, rank=8)** | 28GB+ | 1 | N/A | âŒ OOM |
| **å„ªåŒ– (128, rank=4)** | **13GB** | 1 | 3.79 it/s | âœ… **æˆåŠŸ** |
| **Ultra (64, rank=2)** | ~10GB | 1 | 4.5 it/s | âœ… æ›´å®‰å…¨ |
| **Quality (256, rank=8)** | ~18GB | 1 | 2.5 it/s | âœ… æ›´å¥½æº–ç¢ºåº¦ |

## ğŸ† çµè«–

**æˆå°±è§£é–**: RTX 4090 å¯ä»¥è¨“ç·´ 7B åƒæ•¸æ¨¡å‹ï¼

**é—œéµæŠ€è¡“**:
1. âœ… Gradient Checkpointingï¼ˆæœ€é‡è¦ï¼‰
2. âœ… åºåˆ—é•·åº¦å„ªåŒ–
3. âœ… LoRA åƒæ•¸èª¿æ•´
4. âœ… è¨˜æ†¶é«”ç®¡ç†ç­–ç•¥

**ä¸‹ä¸€æ­¥**:
- âœ“ åœ¨çœŸå¯¦è³‡æ–™ä¸Šè¨“ç·´
- âœ“ èª¿æ•´è¶…åƒæ•¸å„ªåŒ–æº–ç¢ºåº¦
- âœ“ å¯¦é©—ä¸åŒçš„ LoRA é…ç½®

---

**æ¸¬è©¦æ—¥æœŸ**: 2025-11-02  
**GPU**: NVIDIA RTX 4090 (24GB)  
**å³°å€¼è¨˜æ†¶é«”**: 13.0GB / 24GB  
**ç‹€æ…‹**: âœ… **å®Œå…¨æˆåŠŸ**  
**é…ç½®**: `configs/rtx4090_optimized.yaml`

