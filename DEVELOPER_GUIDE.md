# METAGENE Classification Developer Guide
# é–‹ç™¼è€…èˆ‡é€²éšé…ç½®æŒ‡å—

æœ¬æŒ‡å—åŒ…å«æ‰€æœ‰é€²éšé…ç½®ã€è¶…åƒæ•¸èª¿æ•´ã€å„ªåŒ–æŠ€è¡“å’Œé–‹ç™¼ç›¸é—œè³‡è¨Šã€‚

---

## ğŸ“– ç›®éŒ„

1. [è¶…åƒæ•¸å®Œæ•´æŒ‡å—](#è¶…åƒæ•¸å®Œæ•´æŒ‡å—)
2. [RTX 4090 å„ªåŒ–æŠ€è¡“](#rtx-4090-å„ªåŒ–æŠ€è¡“)
3. [Streaming Training](#streaming-training-å¤§è³‡æ–™é›†)
4. [HuggingFace Tokenizer](#huggingface-tokenizer-æ•´åˆ)
5. [æ¸¬è©¦èˆ‡é©—è­‰](#æ¸¬è©¦èˆ‡é©—è­‰)
6. [åŠŸèƒ½æ­¸å±¬èˆ‡è²¢ç»](#åŠŸèƒ½æ­¸å±¬èˆ‡è²¢ç»)

---

# è¶…åƒæ•¸å®Œæ•´æŒ‡å—

## â­ Top 5 æœ€é—œéµçš„è¶…åƒæ•¸

### 1. `tokenizer.max_length` (æœ€å¤§åºåˆ—é•·åº¦)
- **é è¨­å€¼**: 512 (standard) / 128 (RTX 4090)
- **ç¯„åœ**: 64-2048
- **å½±éŸ¿**: âš¡ **å°è¨˜æ†¶é«”å½±éŸ¿æœ€å¤§**
- **å»ºè­°**:
  - çŸ­ reads (<150bp): `128-256`
  - æ¨™æº– reads (150-300bp): `256-512`
  - é•·åºåˆ— (>300bp): `512-1024`
- **è¨˜æ†¶é«”å½±éŸ¿** (batch_size=1):
  - `128` â†’ 13GB
  - `256` â†’ 16GB
  - `512` â†’ 22GB

---

### 2. `lora.r` (LoRA Rank)
- **é è¨­å€¼**: 8 (standard) / 4 (RTX 4090)
- **ç¯„åœ**: 1-64
- **å½±éŸ¿**: ğŸ§  æ¨¡å‹è¡¨é”èƒ½åŠ›, âš¡ è¨˜æ†¶é«”ä½¿ç”¨
- **å»ºè­°**:
  - å°æ•¸æ“šé›† (<10K): `r=2-4`
  - ä¸­ç­‰æ•¸æ“šé›† (10K-100K): `r=4-8`
  - å¤§æ•¸æ“šé›† (>100K): `r=8-16`
- **è¨˜æ†¶é«”å½±éŸ¿**:
  - `r=2` â†’ 11GB
  - `r=4` â†’ 13GB
  - `r=8` â†’ 16GB
  - `r=16` â†’ 22GB

---

### 3. `training.batch_size` Ã— `grad_accum_steps`
- **é è¨­å€¼**: 128Ã—1 (standard) / 1Ã—8 (RTX 4090)
- **ç¯„åœ**: batch_size 1-512, grad_accum 1-32
- **å½±éŸ¿**: ğŸ¯ è¨“ç·´ç©©å®šæ€§, âš¡ è¨˜æ†¶é«”ä½¿ç”¨
- **å»ºè­°æœ‰æ•ˆæ‰¹æ¬¡å¤§å°**:
  - å°æ•¸æ“šé›†: `8-16`
  - æ¨™æº–: `32-64` âœ…
  - å¤§æ•¸æ“šé›†: `64-128`
- **RTX 4090**: `batch_size=1`, `grad_accum=32` (æœ‰æ•ˆæ‰¹æ¬¡=32)

---

### 4. `optimizer.lr` (å­¸ç¿’ç‡)
- **é è¨­å€¼**: 0.0002 (2e-4)
- **ç¯„åœ**: 1e-5 åˆ° 5e-4
- **å½±éŸ¿**: ğŸ¯ æ”¶æ–‚é€Ÿåº¦, ğŸ¯ æœ€çµ‚æ€§èƒ½
- **å»ºè­°**:
  - å°æ•¸æ“šé›† (<10K): `5e-5` åˆ° `1e-4`
  - æ¨™æº– (10K-100K): `1e-4` åˆ° `2e-4` âœ…
  - å¤§æ•¸æ“šé›† (>100K): `2e-4` åˆ° `5e-4`
  - å¾®èª¿å·²è¨“ç·´æ¨¡å‹: `1e-5` åˆ° `5e-5`

---

### 5. `lora.target_modules` (LoRA æ‡‰ç”¨çš„å±¤)
- **é è¨­å€¼**: `[q_proj, k_proj, v_proj, o_proj]` / `[q_proj, v_proj]`
- **å¯é¸**: q_proj, k_proj, v_proj, o_proj
- **å½±éŸ¿**: ğŸ§  æ¨¡å‹å®¹é‡, âš¡ è¨˜æ†¶é«”ä½¿ç”¨
- **å»ºè­°**:
  - è¨˜æ†¶é«”å……è¶³ (>40GB): `[q, k, v, o]`
  - æ¨™æº– (32GB): `[q, v, o]`
  - RTX 4090 (24GB): `[q, v]` âœ…
  - æ¥µè‡´çœè¨˜æ†¶é«”: `[q]`
- **è¨˜æ†¶é«”å½±éŸ¿**: `[q]`â†’-30%, `[q,v]`â†’åŸºæº–, `[q,v,o]`â†’+20%, `[q,k,v,o]`â†’+40%

---

## ğŸ›ï¸ å…¶ä»–é‡è¦è¶…åƒæ•¸

### æ¨¡å‹æ¶æ§‹

| åƒæ•¸ | é è¨­å€¼ | ç¯„åœ | å»ºè­° |
|------|--------|------|------|
| `lora.alpha` | 16 / 8 | råˆ°2*r | alpha = 2*r |
| `lora.dropout` | 0.1 / 0.05 | 0.0-0.3 | 0.05-0.1 |
| `model.dropout` | 0.1 | 0.0-0.5 | 0.1-0.3 |
| `gradient_checkpointing` | false / true | bool | true (24GB), false (40GB+) |
| `model.pooling` | mean | mean/max/cls | mean (æ¨è–¦) |

### è¨“ç·´ç­–ç•¥

| åƒæ•¸ | é è¨­å€¼ | ç¯„åœ | å»ºè­° |
|------|--------|------|------|
| `training.max_epochs` | 10 | 1-100 | 10-20 |
| `optimizer.weight_decay` | 0.01 | 0.0-0.1 | 0.01-0.05 |
| `scheduler.warmup_steps` | 100 / 50 | 0-1000 | ç¸½æ­¥æ•¸çš„1-5% |
| `loss.label_smoothing` | 0.0 | 0.0-0.3 | 0.0-0.1 |
| `early_stopping.patience` | 3 | 1-10 | 3-5 |
| `training.precision` | bf16-mixed | bf16/fp16/32 | bf16-mixed âœ… |

---

## ğŸ’¾ æŒ‰ GPU è¨˜æ†¶é«”å¤§å°çš„æ¨è–¦é…ç½®

| GPU è¨˜æ†¶é«” | batchÃ—accum | max_length | lora.r | target_modules | gradient_ckpt |
|-----------|-------------|------------|--------|----------------|--------------|
| 12GB | 1Ã—32 | 64-128 | 2 | [q] | true |
| 16GB | 1Ã—32 | 128-256 | 2-4 | [q,v] | true |
| **24GB (4090)** | **1Ã—32** | **128-256** | **4-8** | **[q,v]** | **true** âœ… |
| 32GB | 8Ã—4 | 256-512 | 8 | [q,v,o] | false |
| 40GB (A100) | 32Ã—2 | 512 | 8-16 | [q,k,v,o] | false |
| 80GB (A100) | 64Ã—2 | 512-1024 | 16-32 | [q,k,v,o] | false |

---

## ğŸ“Š æŒ‰æ•¸æ“šé›†å¤§å°çš„æ¨è–¦é…ç½®

| æ•¸æ“šé›† | max_epochs | lr | weight_decay | dropout | patience |
|--------|-----------|-----|--------------|---------|----------|
| <1K | 50-100 | 5e-5 | 0.1 | 0.3-0.5 | 2-3 |
| 1K-10K | 20-50 | 1e-4 | 0.05 | 0.2-0.3 | 3 |
| **10K-100K** | **10-20** | **2e-4** | **0.01** | **0.1-0.2** | **3-5** âœ… |
| 100K-1M | 5-10 | 2e-4 | 0.01 | 0.1 | 3-5 |
| >1M | 3-5 | 3e-4 | 0.001 | 0.1 | 5-10 |

---

## ğŸš¨ å¸¸è¦‹å•é¡Œå¿«é€Ÿè¨ºæ–·

### CUDA Out of Memory
**è§£æ±º**: 
- â†“ max_length (æœ€æœ‰æ•ˆï¼)
- gradient_checkpointing=true
- â†“ batch_size
- â†“ lora.r
- â†“ target_modules

### è¨“ç·´å¤ªæ…¢ (<1 it/s)
**è§£æ±º**:
- â†“ max_length
- gradient_checkpointing=false
- â†‘ batch_size
- precision=bf16-mixed

### æ¨¡å‹éæ“¬åˆ (val loss â†‘)
**è§£æ±º**:
- â†‘ dropout (0.2-0.3)
- â†‘ weight_decay (0.05)
- label_smoothing=0.1
- early_stopping

### æ¨¡å‹æ¬ æ“¬åˆ (train/val loss éƒ½é«˜)
**è§£æ±º**:
- â†‘ lora.r
- â†‘ max_length
- â†‘ max_epochs
- â†‘ lr

### è¨“ç·´ä¸ç©©å®š (loss éœ‡ç›ª)
**è§£æ±º**:
- â†“ lr
- â†‘ warmup_steps
- precision=32
- â†‘ grad_accum_steps

---

## ğŸ¯ æŒ‰è¨“ç·´ç›®æ¨™çš„é…ç½®

| ç›®æ¨™ | é—œéµåƒæ•¸èª¿æ•´ |
|------|-------------|
| **æœ€å¿«æ”¶æ–‚** | â†‘ lr, â†‘ batch_size, â†“ warmup_steps |
| **æœ€ä½³æ€§èƒ½** | â†‘ lora.r, â†‘ max_length, â†‘ max_epochs |
| **é˜²æ­¢éæ“¬åˆ** | â†‘ dropout, â†‘ weight_decay, â†‘ label_smoothing, early stopping |
| **æœ€çœè¨˜æ†¶é«”** | â†“ batch_size, â†“ max_length, â†“ lora.r, â†“ target_modules, gradient_checkpointing=true |
| **æœ€å¿«è¨“ç·´** | â†‘ batch_size, gradient_checkpointing=false, precision=bf16-mixed |

---

# RTX 4090 å„ªåŒ–æŠ€è¡“

## ğŸ‰ é‡å¤§çªç ´

**RTX 4090 (24GB) ç¾åœ¨å¯ä»¥æˆåŠŸè¨“ç·´ METAGENE-1 (7B åƒæ•¸)ï¼**

- **å³°å€¼ GPU ä½¿ç”¨**: 13.0GB / 24GB âœ“
- **ç‹€æ…‹**: âœ… ç„¡ OOM éŒ¯èª¤

---

## ğŸ”§ é—œéµå„ªåŒ–ç­–ç•¥

### 1. Gradient Checkpointing â­â­â­ (æœ€é—œéµ)
```yaml
model:
  gradient_checkpointing: true
```
- **æ•ˆæœ**: ç¯€çœ ~50% activation memory
- **æ¬Šè¡¡**: è¨“ç·´é€Ÿåº¦é™ä½ ~15-20%
- **å¯¦ç¾**:
```python
def _enable_gradient_checkpointing(self):
    self.encoder.enable_input_require_grads()
    self.encoder.base_model.gradient_checkpointing_enable()
```

---

### 2. æ¸›å°‘åºåˆ—é•·åº¦
```yaml
tokenizer:
  max_length: 128  # å¾ 512 é™è‡³ 128
```
- **æ•ˆæœ**: ç¯€çœ ~60% sequence memory
- **æ¬Šè¡¡**: é•·åºåˆ—æœƒè¢«æˆªæ–·

---

### 3. æ›´å°çš„ LoRA Rank
```yaml
model:
  lora:
    r: 4  # å¾ 8 é™è‡³ 4
    alpha: 8
```
- **æ•ˆæœ**: ç¯€çœ ~50% LoRA parameters
- **æ¬Šè¡¡**: æ¨¡å‹è¡¨é”èƒ½åŠ›ç•¥é™ï¼ˆé€šå¸¸ <2%ï¼‰

---

### 4. æ¸›å°‘ Target Modules
```yaml
model:
  lora:
    target_modules: [q_proj, v_proj]  # åªè¨“ç·´ Q å’Œ V
```
- **æ•ˆæœ**: ç¯€çœ ~50% adapter memory
- **æ¬Šè¡¡**: ç•¥å¾®é™ä½å¾®èª¿éˆæ´»æ€§

---

### 5. Gradient Accumulation
```yaml
training:
  batch_size: 1
  grad_accum_steps: 8  # æœ‰æ•ˆ batch size = 8
```
- **æ•ˆæœ**: å…è¨±å° batch size åŒæ™‚ä¿æŒè¨“ç·´ç©©å®šæ€§
- **æ¬Šè¡¡**: è¨“ç·´é€Ÿåº¦ç•¥æ…¢

---

### 6. å®šæœŸè¨˜æ†¶é«”æ¸…ç†
```yaml
memory_optimization:
  empty_cache_steps: 10
```
- **æ•ˆæœ**: æ¸›å°‘è¨˜æ†¶é«”ç¢ç‰‡
- **æ¬Šè¡¡**: è¼•å¾®æ€§èƒ½é–‹éŠ·

---

### 7. è¨˜æ†¶é«”åˆ†é…å„ªåŒ–
```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
```
- **æ•ˆæœ**: æ›´å¥½çš„è¨˜æ†¶é«”ç®¡ç†
- **æ¬Šè¡¡**: ç„¡

---

## ğŸ“Š å„ªåŒ–æ•ˆæœç¸½çµ

| å„ªåŒ– | è¨˜æ†¶é«”ç¯€çœ | æ€§èƒ½å½±éŸ¿ | å„ªå…ˆç´š |
|------|-----------|---------|--------|
| Gradient Checkpointing | ~50% | -15~20% é€Ÿåº¦ | â­â­â­â­â­ |
| max_length: 512â†’128 | ~60% | é•·åºåˆ—æˆªæ–· | â­â­â­â­â­ |
| lora.r: 8â†’4 | ~50% | -1~2% æ€§èƒ½ | â­â­â­â­ |
| target_modules: 4â†’2 | ~50% | ç•¥é™ | â­â­â­ |
| Empty cache | ~5% | <1% é€Ÿåº¦ | â­â­ |

**ç¸½æ•ˆæœ**: 28GB â†’ 13GB (ç¯€çœ 54%ï¼)

---

# Streaming Training (å¤§è³‡æ–™é›†)

å¦‚æœæ‚¨çš„è³‡æ–™é›†å¤ªå¤§ï¼ˆ>100GBï¼‰ï¼Œç„¡æ³•ä¸€æ¬¡è¼‰å…¥è¨˜æ†¶é«”ï¼Œå¯ä»¥ä½¿ç”¨ streaming è¨“ç·´ã€‚

## å¯¦ç¾ Streaming Dataset

```python
# modules/dataloading_streaming.py

class StreamingSequenceDataset(Dataset):
    """è¨˜æ†¶é«”é«˜æ•ˆçš„ streaming dataset"""
    
    def __init__(self, fasta_path, tokenizer, mapping_df, max_length=512):
        self.fasta_path = fasta_path
        self.tokenizer = tokenizer
        self.mapping_df = mapping_df
        self.max_length = max_length
        
        # åªå„²å­˜ç´¢å¼•ï¼Œä¸è¼‰å…¥è³‡æ–™
        self.index = self._build_index()
    
    def _build_index(self):
        """å»ºç«‹æª”æ¡ˆä½ç½®ç´¢å¼•"""
        index = []
        with open(self.fasta_path, 'r') as f:
            while True:
                pos = f.tell()
                line = f.readline()
                if not line:
                    break
                if line.startswith('>'):
                    index.append(pos)
        return index
    
    def __getitem__(self, idx):
        """å³æ™‚è®€å–ä¸¦è™•ç†åºåˆ—"""
        pos = self.index[idx]
        with open(self.fasta_path, 'r') as f:
            f.seek(pos)
            header = f.readline().strip()
            sequence = f.readline().strip()
        
        # è§£æä¸¦è¿”å›
        return self._process_sequence(header, sequence)
```

---

## ä½¿ç”¨ Streaming Dataset

```python
# åœ¨ train.py ä¸­

from modules.dataloading_streaming import StreamingSequenceDataset

# æ›¿æ›æ¨™æº– dataset
train_dataset = StreamingSequenceDataset(
    fasta_path=args.train_fasta,
    tokenizer=tokenizer,
    mapping_df=mapping_df,
    max_length=config['tokenizer']['max_length']
)

# ä½¿ç”¨ DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=config['training']['batch_size'],
    shuffle=False,  # Streaming ä¸æ”¯æŒ shuffle
    num_workers=4,   # å¤šé€²ç¨‹è¼‰å…¥
    pin_memory=True
)
```

---

## Streaming çš„å„ªç¼ºé»

### å„ªé» âœ…
- å¯è™•ç†ä»»æ„å¤§å°çš„è³‡æ–™é›†
- è¨˜æ†¶é«”ä½¿ç”¨é‡æ¥µä½
- å•Ÿå‹•æ™‚é–“å¿«ï¼ˆç„¡éœ€é è¼‰å…¥ï¼‰

### ç¼ºé» âŒ
- ç„¡æ³•ä½¿ç”¨ shuffleï¼ˆæˆ–éœ€è¦è¤‡é›œå¯¦ç¾ï¼‰
- I/O å¯èƒ½æˆç‚ºç“¶é ¸
- éœ€è¦è‰¯å¥½çš„æª”æ¡ˆç³»çµ±æ€§èƒ½

---

## æœ€ä½³å¯¦è¸

1. **ä½¿ç”¨ SSD**: Streaming éœ€è¦å¿«é€Ÿ I/O
2. **é å…ˆ shuffle è³‡æ–™**: åœ¨å‰µå»º FASTA æª”æ¡ˆæ™‚å°±æ‰“äº‚
3. **ä½¿ç”¨å¤šé€²ç¨‹**: `num_workers=4-8`
4. **é å–**: `prefetch_factor=2`

```python
DataLoader(
    dataset,
    num_workers=8,      # å¤šé€²ç¨‹
    prefetch_factor=2,  # é å–
    persistent_workers=True  # ä¿æŒ workers æ´»èº
)
```

---

# HuggingFace Tokenizer æ•´åˆ

## å…©ç¨® Tokenizer æ¨¡å¼

### æ¨¡å¼ 1: minbpe Tokenizer (æœ¬åœ°)
```yaml
tokenizer:
  name_or_path: /path/to/minbpe/tokenizer.model
  use_hf_tokenizer: false
```
- è©å½™è¡¨: 1025 tokens
- éœ€è¦æœ¬åœ°æª”æ¡ˆ

### æ¨¡å¼ 2: HuggingFace Tokenizer (æ¨è–¦)
```yaml
tokenizer:
  name_or_path: metagene-ai/METAGENE-1
  use_hf_tokenizer: true
```
- è©å½™è¡¨: 1024 tokens
- èˆ‡æ¨¡å‹å®Œç¾åŒ¹é…
- è‡ªå‹•ä¸‹è¼‰

---

## å¯¦ç¾ç´°ç¯€

```python
# modules/dataloading.py

class MetaGeneTokenizer:
    def __init__(self, tokenizer_path, use_hf_tokenizer=False, max_length=512):
        if use_hf_tokenizer or tokenizer_path.startswith("metagene-ai"):
            # HuggingFace tokenizer
            from transformers import AutoTokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
            self.is_hf = True
        else:
            # minbpe tokenizer
            from minbpe import RegexTokenizer
            self.tokenizer = RegexTokenizer()
            self.tokenizer.load(tokenizer_path)
            self.is_hf = False
    
    def encode(self, text):
        if self.is_hf:
            return self.tokenizer(text, add_special_tokens=False)['input_ids']
        else:
            return self.tokenizer.encode(text, add_special_tokens=False)
```

---

## åˆ‡æ› Tokenizer

### æ–¹æ³• 1: ä¿®æ”¹é…ç½®æ–‡ä»¶
```yaml
# configs/default.yaml
tokenizer:
  name_or_path: metagene-ai/METAGENE-1
  use_hf_tokenizer: true
```

### æ–¹æ³• 2: å‘½ä»¤åˆ—åƒæ•¸
```bash
python train.py \
  --config configs/default.yaml \
  --tokenizer_name_or_path metagene-ai/METAGENE-1 \
  --use_hf_tokenizer true \
  ...
```

---

## æ¯”è¼ƒ

| ç‰¹æ€§ | minbpe | HuggingFace |
|------|--------|-------------|
| è©å½™è¡¨å¤§å° | 1025 | 1024 |
| èˆ‡æ¨¡å‹åŒ¹é… | âš ï¸ ç•¥æœ‰å·®ç•° | âœ… å®Œç¾ |
| éœ€è¦æœ¬åœ°æª”æ¡ˆ | âœ… | âŒ |
| ä¸‹è¼‰ | æ‰‹å‹• | è‡ªå‹• |
| **æ¨è–¦** | | âœ… |

---

# æ¸¬è©¦èˆ‡é©—è­‰

## å–®å…ƒæ¸¬è©¦

```bash
# é‹è¡Œæ‰€æœ‰æ¸¬è©¦
pytest tests/

# é‹è¡Œç‰¹å®šæ¸¬è©¦
pytest tests/test_pipeline.py::test_dataset_loading

# è©³ç´°è¼¸å‡º
pytest -v tests/
```

---

## å¿«é€Ÿæ¸¬è©¦è…³æœ¬

### 1. åªæ¸¬è©¦è³‡æ–™è¼‰å…¥
```bash
python test_dataloader_only.py
```
- ä¸éœ€è¦ GPU
- ä¸ä¸‹è¼‰æ¨¡å‹
- å¿«é€Ÿé©—è­‰è³‡æ–™æ ¼å¼

### 2. å®Œæ•´ Pipeline æ¸¬è©¦
```bash
bash test_optimized_training.sh
```
- ä½¿ç”¨ç¯„ä¾‹è³‡æ–™
- 1 epoch å¿«é€Ÿæ¸¬è©¦
- ~3 åˆ†é˜

---

## è³‡æ–™æ ¼å¼é©—è­‰

```bash
python test_data_format.py --fasta data/train.fa
```

æª¢æŸ¥ï¼š
- Header æ ¼å¼æ˜¯å¦æ­£ç¢º
- class_id æ˜¯å¦åœ¨ mapping ä¸­
- åºåˆ—æ˜¯å¦æœ‰æ•ˆï¼ˆåªå« ACGT ç­‰ï¼‰

---

## Pre-training Checklist

```bash
bash pre_training_checklist.sh
```

æª¢æŸ¥ï¼š
- âœ… è³‡æ–™æª”æ¡ˆå­˜åœ¨
- âœ… Mapping æª”æ¡ˆæ ¼å¼æ­£ç¢º
- âœ… GPU å¯ç”¨ä¸”è¨˜æ†¶é«”è¶³å¤ 
- âœ… ç’°å¢ƒè®Šæ•¸è¨­ç½®æ­£ç¢º
- âœ… ç£ç¢Ÿç©ºé–“è¶³å¤ 

---

# åŠŸèƒ½æ­¸å±¬èˆ‡è²¢ç»

## ğŸ”µ METAGENE æä¾›çš„çµ„ä»¶

### 1. METAGENE-1 æ¨¡å‹
- **ä¾†æº**: [HuggingFace](https://huggingface.co/metagene-ai/METAGENE-1)
- **å¤§å°**: 7B åƒæ•¸
- **ç”¨é€”**: åºåˆ— encoder
- **ä¿®æ”¹**: âŒ ç„¡ï¼Œå®Œå…¨ä½¿ç”¨åŸæ¨¡å‹

### 2. å®˜æ–¹ Tokenizer
- **ä¾†æº**: HuggingFace - metagene-ai/METAGENE-1
- **è©å½™è¡¨**: 1024 tokens
- **ä¿®æ”¹**: âŒ ç„¡

---

## ğŸŸ¢ æˆ‘å€‘æ–°å¢çš„çµ„ä»¶

### 1. Classification Pipeline â­
```python
METAGENE-1 Encoder (å‡çµ)
    â†“
Mean Pooling (æˆ‘å€‘åŠ çš„)
    â†“
Linear Classifier (æˆ‘å€‘åŠ çš„)
```
- **æª”æ¡ˆ**: `modules/modeling.py`
- **åŸå‰µæ€§**: âœ… 100%

### 2. LoRA Fine-tuning æ•´åˆ
- **æª”æ¡ˆ**: `modules/modeling.py` - `_setup_lora()`
- **ä½¿ç”¨**: HuggingFace PEFT åº«
- **åŸå‰µæ€§**: âœ… æˆ‘å€‘çš„å¯¦ç¾

### 3. Gradient Checkpointing â­â­â­
- **æª”æ¡ˆ**: `modules/modeling.py` - `_enable_gradient_checkpointing()`
- **é—œéµå‰µæ–°**: ä½¿ RTX 4090 èƒ½è¨“ç·´ 7B æ¨¡å‹
- **æ•ˆæœ**: ç¯€çœ 50% activation memory
- **åŸå‰µæ€§**: âœ… æˆ‘å€‘çš„å¯¦ç¾

### 4. Data Loading Pipeline
- **æª”æ¡ˆ**: `modules/dataloading.py`
- **åŠŸèƒ½**: FASTA/FASTQ è§£æ, Header regex, Label mapping
- **åŸå‰µæ€§**: âœ… 100%

### 5. Training Pipeline
- **æª”æ¡ˆ**: `train.py`, `evaluate.py`, `predict.py`
- **åŠŸèƒ½**: å®Œæ•´çš„è¨“ç·´ã€è©•ä¼°å’Œæ¨ç†ç³»çµ±
- **åŸå‰µæ€§**: âœ… 100%

### 6. RTX 4090 å„ªåŒ–é…ç½® â­â­â­
- **æª”æ¡ˆ**: `configs/rtx4090_optimized.yaml`
- **æ•ˆæœ**: 13GB / 24GBï¼ˆæˆåŠŸï¼ï¼‰
- **åŸå‰µæ€§**: âœ… å®Œå…¨åŸå‰µ

### 7. æ‰€æœ‰æ–‡æª”
- **æª”æ¡ˆ**: USER_GUIDE.md, DEVELOPER_GUIDE.md, README.md ç­‰
- **åŸå‰µæ€§**: âœ… 100%

---

## ğŸ“Š è²¢ç»æ¯”ä¾‹ç¸½çµ

| æ¨¡å¡Š | METAGENE æä¾› | æˆ‘å€‘æ–°å¢ |
|------|--------------|---------|
| **æ¨¡å‹æ¬Šé‡** | 100% | 0% |
| **Tokenizer** | 100% | 0% |
| **Classification Head** | 0% | 100% |
| **LoRA æ•´åˆ** | 0% | 100% |
| **Gradient Checkpointing** | 0% | 100% â­ |
| **Data Loading** | 0% | 100% |
| **Training Pipeline** | 0% | 100% |
| **RTX 4090 å„ªåŒ–** | 0% | 100% â­â­â­ |
| **æ–‡æª”** | 0% | 100% |

---

## ğŸ¯ æˆ‘å€‘çš„ä¸»è¦å‰µæ–°

### 1. ğŸ† RTX 4090 æ”¯æ´
- **å•é¡Œ**: 7B æ¨¡å‹éœ€è¦ 28GBï¼ŒRTX 4090 åªæœ‰ 24GB
- **è§£æ±º**: Gradient checkpointing + åºåˆ—é•·åº¦å„ªåŒ– + LoRA é…ç½®
- **çµæœ**: 13GB / 24GBï¼ˆæˆåŠŸï¼ï¼‰
- **é€™æ˜¯æœ€å¤§çš„å‰µæ–°**

### 2. ğŸ”§ å®Œæ•´çš„ Classification Pipeline
- METAGENE-1 åªæ˜¯ encoder
- æˆ‘å€‘å»ºç«‹äº†å®Œæ•´çš„è¨“ç·´ã€è©•ä¼°ã€æ¨ç†ç³»çµ±

### 3. ğŸ“š Production-Ready æ–‡æª”
- å¾å®‰è£åˆ°éƒ¨ç½²çš„å®Œæ•´æŒ‡å—
- å¤šå€‹é…ç½®å’Œæ¸¬è©¦è…³æœ¬

---

## ğŸ“ ä½¿ç”¨è²æ˜å»ºè­°

å¦‚æœè¦ç™¼è¡¨æˆ–åˆ†äº«é€™å€‹å·¥ä½œï¼Œå»ºè­°ï¼š

```
æœ¬åˆ†é¡ pipeline åŸºæ–¼ METAGENE-1 é è¨“ç·´æ¨¡å‹ï¼ˆç”± metagene-ai é–‹ç™¼ï¼‰æ§‹å»ºã€‚
æˆ‘å€‘å¯¦ç¾äº†ï¼š
- å®Œæ•´çš„åˆ†é¡æ¶æ§‹ï¼ˆpooling + classifierï¼‰
- LoRA å¾®èª¿æ•´åˆ
- RTX 4090 è¨˜æ†¶é«”å„ªåŒ–ï¼ˆgradient checkpointing ç­‰ï¼‰
- å®Œæ•´çš„è¨“ç·´ã€è©•ä¼°å’Œæ¨ç† pipeline
- ç”Ÿç”¢ç´šæ–‡æª”å’Œæ¸¬è©¦å¥—ä»¶

METAGENE-1 æ¨¡å‹è«‹å¼•ç”¨ï¼š
Liu et al. (2025). METAGENE-1: Metagenomic Foundation Model 
for Pandemic Monitoring. arXiv:2501.02045
```

---

## ğŸ” æŠ€è¡“æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æˆ‘å€‘çš„ Classification System              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  [æˆ‘å€‘çš„] Data Loading Pipeline                              â”‚
â”‚      â†“                                                       â”‚
â”‚  [METAGENE] Tokenizer (å®˜æ–¹ or minbpe)                       â”‚
â”‚      â†“                                                       â”‚
â”‚  [METAGENE] METAGENE-1 Encoder (7B, å‡çµ)                   â”‚
â”‚      â†“                                                       â”‚
â”‚  [æˆ‘å€‘çš„] LoRA Adapters (åªè¨“ç·´é€™äº›)                          â”‚
â”‚      â†“                                                       â”‚
â”‚  [æˆ‘å€‘çš„] Mean Pooling                                       â”‚
â”‚      â†“                                                       â”‚
â”‚  [æˆ‘å€‘çš„] Linear Classifier                                  â”‚
â”‚      â†“                                                       â”‚
â”‚  [æˆ‘å€‘çš„] Loss & Metrics                                     â”‚
â”‚      â†“                                                       â”‚
â”‚  [æˆ‘å€‘çš„] Training Loop & Optimization                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[æˆ‘å€‘çš„] Gradient Checkpointing â† è²«ç©¿æ•´å€‹ forward pass
[æˆ‘å€‘çš„] Memory Optimization     â† è²«ç©¿æ•´å€‹ training loop
```

---

## ç¸½çµ

### METAGENE æä¾›çš„ï¼š
- âœ… å¼·å¤§çš„é è¨“ç·´ encoderï¼ˆ7B åƒæ•¸ï¼Œ1.5T è³‡æ–™ï¼‰
- âœ… å®˜æ–¹ tokenizerï¼ˆ1024 vocabï¼‰

### æˆ‘å€‘å»ºç«‹çš„ï¼š
- âœ… å®Œæ•´çš„åˆ†é¡ç³»çµ±ï¼ˆå¾è³‡æ–™åˆ°é æ¸¬ï¼‰
- âœ… RTX 4090 å„ªåŒ–ï¼ˆçªç ´æ€§æˆå°±ï¼‰
- âœ… ç”Ÿç”¢ç´šä»£ç¢¼å’Œæ–‡æª”
- âœ… éˆæ´»çš„é…ç½®ç³»çµ±
- âœ… å®Œæ•´çš„æ¸¬è©¦å¥—ä»¶

**æ¯”å–»**: METAGENE-1 å°±åƒä¸€å€‹å¼·å¤§çš„ã€Œç‰¹å¾µæå–å¼•æ“ã€ï¼Œæˆ‘å€‘åœ¨å®ƒä¸Šé¢å»ºç«‹äº†ä¸€å€‹å®Œæ•´çš„ã€Œåˆ†é¡å·¥å» ã€ï¼ŒåŒ…æ‹¬è¼¸å…¥è™•ç†ã€è³ªé‡æ§åˆ¶ã€è¼¸å‡ºåŒ…è£ã€ç”Ÿç”¢ç·šå„ªåŒ–ç­‰æ‰€æœ‰ç’°ç¯€ã€‚

---

**æœ€å¾Œæ›´æ–°**: 2025-11-10  
**ç‰ˆæœ¬**: 2.0 (æ•´åˆç‰ˆ)

