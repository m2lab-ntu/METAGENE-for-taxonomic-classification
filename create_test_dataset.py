#!/usr/bin/env python3
"""
å‰µå»ºç¨ç«‹çš„æ¸¬è©¦æ•¸æ“šé›†ï¼Œç”¨æ–¼è©•ä¼°ä¸åŒæ–¹æ³•çš„æ€§èƒ½
Creates an independent test dataset for evaluating different methods

ç‰¹é» (Features):
1. ç¢ºä¿èˆ‡è¨“ç·´/é©—è­‰é›†ä¸é‡ç–Š (No overlap with train/val sets)
2. æ¯å€‹ç‰©ç¨®æ¡æ¨£å›ºå®šæ•¸é‡çš„è®€ (Fixed number of reads per species)
3. ä¿ç•™ ground truth æ¨™ç±¤ (Keep ground truth labels)
4. æ”¯æŒå¤šç¨®æ¡æ¨£ç­–ç•¥ (Support multiple sampling strategies)
"""

import os
import sys
import random
import argparse
from pathlib import Path
from collections import defaultdict
from tqdm import tqdm
import gzip


def parse_fasta_header(header):
    """è§£æ FASTA header: >lbl|class_id|tax_id|length|species_name"""
    parts = header.strip('>').split('|')
    if len(parts) >= 5:
        return {
            'class_id': parts[1],
            'tax_id': parts[2],
            'length': parts[3],
            'species_name': parts[4]
        }
    return None


def read_fasta_sequences(fasta_file, max_reads_per_file=None):
    """è®€å– FASTA æ–‡ä»¶ä¸­çš„åºåˆ—"""
    sequences = []
    
    if fasta_file.endswith('.gz'):
        opener = lambda f: gzip.open(f, 'rt')
    else:
        opener = open
    
    with opener(fasta_file) as f:
        header = None
        sequence_lines = []
        
        for line in f:
            line = line.strip()
            if line.startswith('>'):
                if header and sequence_lines:
                    sequences.append({
                        'header': header,
                        'sequence': ''.join(sequence_lines)
                    })
                    if max_reads_per_file and len(sequences) >= max_reads_per_file:
                        break
                header = line
                sequence_lines = []
            else:
                sequence_lines.append(line)
        
        # æ·»åŠ æœ€å¾Œä¸€æ¢åºåˆ—
        if header and sequence_lines:
            sequences.append({
                'header': header,
                'sequence': ''.join(sequence_lines)
            })
    
    return sequences


def get_train_val_read_ids(train_dir, val_dir):
    """ç²å–è¨“ç·´å’Œé©—è­‰é›†ä¸­çš„æ‰€æœ‰è®€ IDï¼ˆé¿å…é‡ç–Šï¼‰"""
    read_ids = set()
    
    print("ğŸ“– è®€å–è¨“ç·´é›†å’Œé©—è­‰é›†çš„åºåˆ— ID...")
    
    for data_dir, name in [(train_dir, "è¨“ç·´é›†"), (val_dir, "é©—è­‰é›†")]:
        if not os.path.exists(data_dir):
            print(f"âš ï¸  {name} ç›®éŒ„ä¸å­˜åœ¨: {data_dir}")
            continue
            
        files = [f for f in os.listdir(data_dir) if f.endswith('.fa') or f.endswith('.fasta')]
        
        for filename in tqdm(files, desc=f"è™•ç†{name}"):
            filepath = os.path.join(data_dir, filename)
            sequences = read_fasta_sequences(filepath)
            
            for seq in sequences:
                # ä½¿ç”¨ header + sequence ä½œç‚ºå”¯ä¸€ ID
                read_id = f"{seq['header']}_{seq['sequence'][:50]}"
                read_ids.add(read_id)
    
    print(f"âœ… å…±æ‰¾åˆ° {len(read_ids):,} æ¢è¨“ç·´/é©—è­‰åºåˆ—")
    return read_ids


def create_test_dataset(
    source_dir,
    output_file,
    train_dir=None,
    val_dir=None,
    reads_per_species=100,
    max_species=None,
    min_sequence_length=50,
    seed=42
):
    """
    å‰µå»ºæ¸¬è©¦æ•¸æ“šé›†
    
    Args:
        source_dir: æºæ•¸æ“šç›®éŒ„ (full_labeled_species_sequences)
        output_file: è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
        train_dir: è¨“ç·´é›†ç›®éŒ„ï¼ˆç”¨æ–¼æª¢æŸ¥é‡ç–Šï¼‰
        val_dir: é©—è­‰é›†ç›®éŒ„ï¼ˆç”¨æ–¼æª¢æŸ¥é‡ç–Šï¼‰
        reads_per_species: æ¯å€‹ç‰©ç¨®æ¡æ¨£çš„è®€æ•¸
        max_species: æœ€å¤§ç‰©ç¨®æ•¸ï¼ˆNone = å…¨éƒ¨ï¼‰
        min_sequence_length: æœ€å°åºåˆ—é•·åº¦
        seed: éš¨æ©Ÿç¨®å­
    """
    random.seed(seed)
    
    # ç²å–è¨“ç·´/é©—è­‰é›†çš„è®€ IDï¼ˆç”¨æ–¼å»é‡ï¼‰
    existing_read_ids = set()
    if train_dir or val_dir:
        existing_read_ids = get_train_val_read_ids(train_dir or "", val_dir or "")
    
    print(f"\nğŸ”¬ å‰µå»ºæ¸¬è©¦æ•¸æ“šé›†...")
    print(f"   æºç›®éŒ„: {source_dir}")
    print(f"   è¼¸å‡ºæ–‡ä»¶: {output_file}")
    print(f"   æ¯ç‰©ç¨®è®€æ•¸: {reads_per_species}")
    print(f"   æœ€å°åºåˆ—é•·åº¦: {min_sequence_length}")
    
    # ç²å–æ‰€æœ‰ç‰©ç¨®æ–‡ä»¶
    species_files = [f for f in os.listdir(source_dir) if f.endswith('.fa')]
    
    if max_species:
        species_files = random.sample(species_files, min(max_species, len(species_files)))
    
    print(f"   è™•ç†ç‰©ç¨®æ•¸: {len(species_files)}")
    
    # ç‚ºæ¯å€‹ç‰©ç¨®æ¡æ¨£åºåˆ—
    test_sequences = []
    species_stats = defaultdict(lambda: {'total': 0, 'sampled': 0, 'filtered': 0, 'overlap': 0})
    
    for species_file in tqdm(species_files, desc="æ¡æ¨£åºåˆ—"):
        species_path = os.path.join(source_dir, species_file)
        sequences = read_fasta_sequences(species_path)
        
        species_name = species_file.replace('.fa', '')
        species_stats[species_name]['total'] = len(sequences)
        
        # éæ¿¾åºåˆ—
        valid_sequences = []
        for seq in sequences:
            # æª¢æŸ¥é•·åº¦
            if len(seq['sequence']) < min_sequence_length:
                species_stats[species_name]['filtered'] += 1
                continue
            
            # æª¢æŸ¥æ˜¯å¦èˆ‡è¨“ç·´/é©—è­‰é›†é‡ç–Š
            read_id = f"{seq['header']}_{seq['sequence'][:50]}"
            if read_id in existing_read_ids:
                species_stats[species_name]['overlap'] += 1
                continue
            
            valid_sequences.append(seq)
        
        # æ¡æ¨£
        num_to_sample = min(reads_per_species, len(valid_sequences))
        sampled = random.sample(valid_sequences, num_to_sample)
        
        species_stats[species_name]['sampled'] = len(sampled)
        test_sequences.extend(sampled)
    
    # æ‰“äº‚é †åº
    random.shuffle(test_sequences)
    
    # å¯«å…¥è¼¸å‡ºæ–‡ä»¶
    print(f"\nğŸ“ å¯«å…¥æ¸¬è©¦æ•¸æ“šé›†...")
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w') as f:
        for seq in test_sequences:
            f.write(f"{seq['header']}\n")
            f.write(f"{seq['sequence']}\n")
    
    # çµ±è¨ˆä¿¡æ¯
    total_sequences = len(test_sequences)
    total_species = len([s for s in species_stats.values() if s['sampled'] > 0])
    total_filtered = sum(s['filtered'] for s in species_stats.values())
    total_overlap = sum(s['overlap'] for s in species_stats.values())
    
    print(f"\nâœ… æ¸¬è©¦æ•¸æ“šé›†å‰µå»ºå®Œæˆï¼")
    print(f"\nğŸ“Š çµ±è¨ˆä¿¡æ¯:")
    print(f"   ç¸½åºåˆ—æ•¸: {total_sequences:,}")
    print(f"   ç‰©ç¨®æ•¸: {total_species}")
    print(f"   éæ¿¾æ‰çš„åºåˆ— (é•·åº¦ä¸è¶³): {total_filtered:,}")
    print(f"   éæ¿¾æ‰çš„åºåˆ— (èˆ‡è¨“ç·´é›†é‡ç–Š): {total_overlap:,}")
    print(f"   å¹³å‡æ¯ç‰©ç¨®åºåˆ—æ•¸: {total_sequences/total_species:.1f}")
    
    # ä¿å­˜çµ±è¨ˆä¿¡æ¯
    stats_file = output_file.replace('.fa', '_stats.txt').replace('.fasta', '_stats.txt')
    with open(stats_file, 'w') as f:
        f.write(f"æ¸¬è©¦æ•¸æ“šé›†çµ±è¨ˆä¿¡æ¯\n")
        f.write(f"{'='*60}\n\n")
        f.write(f"ç¸½åºåˆ—æ•¸: {total_sequences:,}\n")
        f.write(f"ç‰©ç¨®æ•¸: {total_species}\n")
        f.write(f"éæ¿¾åºåˆ— (é•·åº¦): {total_filtered:,}\n")
        f.write(f"éæ¿¾åºåˆ— (é‡ç–Š): {total_overlap:,}\n")
        f.write(f"å¹³å‡æ¯ç‰©ç¨®: {total_sequences/total_species:.1f}\n\n")
        f.write(f"æ¯å€‹ç‰©ç¨®çš„è©³ç´°çµ±è¨ˆ:\n")
        f.write(f"{'-'*60}\n")
        
        for species, stats in sorted(species_stats.items()):
            if stats['sampled'] > 0:
                f.write(f"{species}: {stats['sampled']}/{stats['total']} "
                       f"(éæ¿¾: {stats['filtered']}, é‡ç–Š: {stats['overlap']})\n")
    
    print(f"   çµ±è¨ˆä¿¡æ¯å·²ä¿å­˜: {stats_file}")
    
    return test_sequences, species_stats


def main():
    parser = argparse.ArgumentParser(
        description="å‰µå»ºç”¨æ–¼æ€§èƒ½æ¯”è¼ƒçš„æ¸¬è©¦æ•¸æ“šé›†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # å‰µå»ºå°å‹æ¸¬è©¦é›†ï¼ˆæ¯ç‰©ç¨® 50 æ¢è®€ï¼Œæœ€å¤š 100 å€‹ç‰©ç¨®ï¼‰
  python create_test_dataset.py \\
    --source_dir /media/user/disk2/full_labeled_species_sequences \\
    --output test_data/test_small.fa \\
    --reads_per_species 50 \\
    --max_species 100

  # å‰µå»ºå®Œæ•´æ¸¬è©¦é›†ï¼Œä¸¦æª¢æŸ¥èˆ‡è¨“ç·´é›†çš„é‡ç–Š
  python create_test_dataset.py \\
    --source_dir /media/user/disk2/full_labeled_species_sequences \\
    --output test_data/test_full.fa \\
    --train_dir /media/user/disk2/full_labeled_species_train_reads_shuffled \\
    --val_dir /media/user/disk2/full_labeled_species_val_reads_shuffled \\
    --reads_per_species 100
        """
    )
    
    parser.add_argument('--source_dir', required=True,
                       help='æºæ•¸æ“šç›®éŒ„')
    parser.add_argument('--output', required=True,
                       help='è¼¸å‡ºæ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--train_dir', default=None,
                       help='è¨“ç·´é›†ç›®éŒ„ï¼ˆç”¨æ–¼æª¢æŸ¥é‡ç–Šï¼‰')
    parser.add_argument('--val_dir', default=None,
                       help='é©—è­‰é›†ç›®éŒ„ï¼ˆç”¨æ–¼æª¢æŸ¥é‡ç–Šï¼‰')
    parser.add_argument('--reads_per_species', type=int, default=100,
                       help='æ¯å€‹ç‰©ç¨®æ¡æ¨£çš„è®€æ•¸ (é è¨­: 100)')
    parser.add_argument('--max_species', type=int, default=None,
                       help='æœ€å¤§ç‰©ç¨®æ•¸ (None = å…¨éƒ¨)')
    parser.add_argument('--min_length', type=int, default=50,
                       help='æœ€å°åºåˆ—é•·åº¦ (é è¨­: 50)')
    parser.add_argument('--seed', type=int, default=42,
                       help='éš¨æ©Ÿç¨®å­ (é è¨­: 42)')
    
    args = parser.parse_args()
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“šé›†
    create_test_dataset(
        source_dir=args.source_dir,
        output_file=args.output,
        train_dir=args.train_dir,
        val_dir=args.val_dir,
        reads_per_species=args.reads_per_species,
        max_species=args.max_species,
        min_sequence_length=args.min_length,
        seed=args.seed
    )


if __name__ == '__main__':
    main()

